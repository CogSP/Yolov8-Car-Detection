{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3866417,"sourceType":"datasetVersion","datasetId":843852}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-08-05T14:30:22.259951Z","iopub.execute_input":"2024-08-05T14:30:22.260633Z","iopub.status.idle":"2024-08-05T14:30:22.266417Z","shell.execute_reply.started":"2024-08-05T14:30:22.260600Z","shell.execute_reply":"2024-08-05T14:30:22.265357Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"class YOLODataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n        # Check if dataset is empty\n        if len(self.img_labels) == 0:\n            raise ValueError(\"Dataset is empty. Please check the CSV file and the image directory.\")\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"Image file {img_path} not found.\")\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Extract bounding box and class probabilities\n        labels = self.img_labels.iloc[idx, 1:].values.astype(float)\n        labels = torch.tensor(labels, dtype=torch.float32)  # Convert labels to Float32\n\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            labels = self.target_transform(labels)\n\n        return {'image': image, 'label': labels}","metadata":{"execution":{"iopub.status.busy":"2024-08-05T14:30:22.267874Z","iopub.execute_input":"2024-08-05T14:30:22.268155Z","iopub.status.idle":"2024-08-05T14:30:22.280671Z","shell.execute_reply.started":"2024-08-05T14:30:22.268132Z","shell.execute_reply":"2024-08-05T14:30:22.279851Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, k, s, p, c=3, dim=64, mc=512, w=1):\n        super(ConvBlock, self).__init__()\n        dim = int(dim)\n        self.conv = nn.Conv2d(in_channels=c, out_channels=min(dim,mc)*w, kernel_size=k, stride=s, padding=p)\n        self.batch_norm = nn.BatchNorm2d(num_features=min(dim,mc)*w)\n        self.activation = nn.SiLU()\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        return x\n    \n\nclass Bottleneck(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, dim=64, shortcut=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = ConvBlock(k,s,p,c,dim=dim)\n        self.conv2 = ConvBlock(k,s,p,c,dim=dim)\n        self.short = shortcut\n    \n    def forward(self, x):\n        if self.short: \n            res = self.conv1(x)\n            res = self.conv2(res)\n            return x + res\n        else: \n            res = self.conv1(x)\n            res = self.conv2(x)\n            return res \n            \nclass C2fBlock(nn.Module):\n    def __init__(self, k=1, s=1, p=0, c=3, depth_multiple=1, shortcut=True, dim=64, mc=512, w=1):\n        super(C2fBlock, self).__init__() \n        self.conv1 = ConvBlock(k,s,p,c,dim=dim,mc=mc,w=w)\n        half_c= int(c / 2)\n        half_dim= int(dim / 2)\n        self.bottlenecks = nn.ModuleList([Bottleneck(k=3,s=1,p=1,c=half_c,dim=half_dim) for _ in range(depth_multiple)])\n        new_input = int(c / 2) * (depth_multiple + 2)\n        self.conv2 = ConvBlock(k,s,p,c=new_input,dim=dim,mc=mc,w=w)\n    \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        # Split the input tensor into two halves along the channel dimension\n        x1, x2 = torch.split(x, x.size(1) // 2, dim=1)\n        \n        \n        # Process the other half (x2) through the bottlenecks\n        bottleneck_outputs = []\n        # append half of the input before processing\n        bottleneck_outputs.append(x2.clone())\n        for bott in self.bottlenecks:\n            x2 = bott(x2)\n            bottleneck_outputs.append(x2.clone())\n            \n        # this will concatenate half of the input before processing\n        # and after each bottleneck processing  \n        \n        concatenated_bottleneck_outputs = torch.cat(bottleneck_outputs, dim=1)\n\n        # add the other half\n        x = torch.cat((x1, concatenated_bottleneck_outputs), dim=1)\n        \n        x = self.conv2(x)\n        return x\n    \nclass SPPF(nn.Module):\n    def __init__(self, k=3, s=1, p=0, c=3, dim=64):\n        super(SPPF, self).__init__() \n        \n        #Le dimensioni dei pool sono sicuramente da modificare\n        self.conv1 = ConvBlock(k=k,s=s,p=0,c=c,dim=dim)\n        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n        self.pool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=4)\n        self.pool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=6)\n        self.conv2 = ConvBlock(k=k,s=s,p=1,c=c,dim=dim)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        pool1 = self.pool1(x)\n        pool2 = self.pool2(x)\n        pool3 = self.pool3(x)\n        x = torch.cat([x, pool1, pool2, pool3], dim=1)\n        x = self.conv2(x)\n        return x\n    \nclass DetectBlock(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, reg_max=16, nc=1, mc=512, w=1):\n        super(DetectBlock, self).__init__()\n        \n        #reg_max = controlla la precisione della regression sulla boundy box \n        #nc = number of classes\n        self.box_conv1 = ConvBlock(k,s,p,c)\n        self.box_conv2 = ConvBlock(k,s,p,c)\n        self.box_conv3 = nn.Conv2d(in_channels=c, out_channels=4*reg_max, kernel_size=k, stride=1, padding=0)\n        \n        self.class_conv1 = ConvBlock(k,s,p,c)\n        self.class_conv2 = ConvBlock(k,s,p,c)\n        self.class_conv3 = nn.Conv2d(in_channels=c, out_channels=nc, kernel_size=k, stride=1, padding=0)\n        \n    def forward(self, x): \n        ret1 = self.box_conv1(x)\n        ret1 = self.box_conv2(ret1)\n        ret1 = self.box_conv3(ret1)\n        \n        ret2 = self.class_conv1(x)\n        ret2 = self.class_conv2(ret2)\n        ret2 = self.class_conv3(ret2)\n        \n        return ret1, ret2\n               \nclass BackBone(nn.Module):\n    def __init__(self, k=3, s=2, p=1, depth=1):\n        super(BackBone, self).__init__()\n        \n        self.conv1 = ConvBlock(k,s,p)\n        self.conv2 = ConvBlock(k,s,p, dim=128, c=64)\n        self.c2f = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=128, c=128)\n        self.conv3 = ConvBlock(k,s,p, dim=256, c=128)\n        self.c2f_second = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=256, c=256)\n        self.conv4 = ConvBlock(k,s,p,dim=512, c=256)\n        self.c2f_third = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=512, c=512)\n        self.conv5 = ConvBlock(k,s,p,dim=1024, c=512)\n        self.c2f_last = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=1024, c=512)\n        \n    def forward(self, x):\n        \n        if DEBUG:\n            print(\"[Layer: Conv 0]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv1(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Conv 1]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv2(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 2]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.c2f(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Conv 3]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")        \n        x = self.conv3(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 4]\")\n            print(f\"Input Tensor Shape:  {x.shape}\") \n        x_first = self.c2f_second(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_first.shape}\")\n\n            print(\"[Layer: Conv 5]\")\n            print(f\"Input Tensor Shape:  {x_first.shape}\") \n        x = self.conv4(x_first)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 6]\")\n            print(f\"Input Tensor Shape:  {x.shape}\") \n        x_second = self.c2f_third(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_second.shape}\")\n\n            print(\"[Layer: Conv 7]\")\n            print(f\"Input Tensor Shape:  {x_second.shape}\")\n        x = self.conv5(x_second)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 8]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x_last = self.c2f_last(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_last.shape}\")\n        \n        return x_first, x_second, x_last\n    \nclass Neck(nn.Module):\n    def __init__(self, depth=1, scale=2):\n        super(Neck, self).__init__()\n        \n        self.sppf = SPPF(k=1,dim=1024,c=512)\n        self.upsample1 = nn.Upsample(size=(24,24))\n        self.upsample2 = nn.Upsample(scale_factor=2)\n        self.c2f_block1 = C2fBlock(dim=512,c=512)\n        self.c2f_block2 = C2fBlock(dim=256,c=768) #256 + 512 check\n        self.c2f_block3 = C2fBlock(dim=512,c=768)\n        self.c2f_block4 = C2fBlock(dim=1024,c=512)\n        self.conv1 = ConvBlock(k=3,s=2,p=1,dim=256,c=256)\n        self.conv2 = ConvBlock(k=3,s=2,p=1,dim=512,c=512)\n        \n    def forward(self, x_first, x_second, x_last):\n        \n        if DEBUG:\n            print(\"[Layer: SPPF 9]\")\n            print(f\"Input Tensor Shape:  {x_last.shape}\")\n        out_sppf = self.sppf(x_last)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {out_sppf.shape}\")\n\n            print(\"[Layer: Upsample 10]\")\n            print(f\"Input Tensor Shape:  {out_sppf.shape}\")\n        x = self.upsample1(out_sppf)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 11]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {x_second.shape}\")\n        x = torch.cat((x,x_second), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n        \n            print(\"[Layer: C2f 12]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        conc1 = self.c2f_block1(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {conc1.shape}\")\n\n            print(\"[Layer: Upsample 13]\")\n            print(f\"Input Tensor Shape:  {conc1.shape}\")\n        x = self.upsample2(conc1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 14]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {x_first.shape}\")\n        x = torch.cat((x,x_first), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 15]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det1 = self.c2f_block2(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det1.shape}\")\n\n            print(\"[Layer: Conv 16]\")\n            print(f\"Input Tensor Shape:  {det1.shape}\")\n        x = self.conv1(det1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 17]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {conc1.shape}\")\n        x = torch.cat((x,conc1), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 18]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det2 = self.c2f_block3(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det2.shape}\")\n\n            print(\"[Layer: Conv 19]\")\n            print(f\"Input Tensor Shape:  {det2.shape}\")\n        x = self.conv2(det2)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 20]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {out_sppf.shape}\")\n        x = torch.cat((x,out_sppf), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 21]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det3 = self.c2f_block4(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det3.shape}\")\n        \n        return det1, det2, det3\n    \nclass Head(nn.Module):\n    def __init__(self):\n        super(Head, self).__init__()\n        \n        self.det1 = DetectBlock()\n        self.det2 = DetectBlock()\n        self.det3 = DetectBlock()\n        \n    def forward(self, x1, x2, x3):\n        return self.det1(x1), self.det2(x2), self.det3(x3)\n    \n    \nclass YOLO(nn.Module):\n    def __init__(self):\n        super(YOLO, self).__init__()\n        self.h1 = BackBone()\n        self.h2 = Neck()\n        self.h3 = Head()\n        \n    def forward(self, x): \n        \n        if DEBUG:\n            print(\"---------- Backbone ----------\")\n            print(\"[Backbone Input]\")\n            print(f\"Input Tensor Shape: {x.shape}\")\n        res1, res2, res3 = self.h1(x)\n        if DEBUG:\n            print(\"[Backbone Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n            print(\"------------------------------\")\n\n            print(\"---------- Neck ----------\")\n            print(\"[Neck Input]\")\n            print(f\"Input Tensor Shape:  \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n        det1, det2, det3 = self.h2(res1, res2, res3)\n        if DEBUG:\n            print(\"[Neck Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n            print(\"------------------------------\")\n\n            print(\"---------- Head ----------\")\n            print(\"[Head Input]\")\n            print(f\"Input Tensor Shape: \\n\\t\\t      {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n        det1, det2, det3 = self.h3(det1, det2, det3)\n        if DEBUG:\n            print(\"[Head Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n            print(\"------------------------------\")\n        \n        return det1, det2, det3\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-05T14:36:31.364167Z","iopub.execute_input":"2024-08-05T14:36:31.364531Z","iopub.status.idle":"2024-08-05T14:36:31.424443Z","shell.execute_reply.started":"2024-08-05T14:36:31.364500Z","shell.execute_reply":"2024-08-05T14:36:31.423231Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Global debug flag\nDEBUG = False  # Set to False to disable debug output\n\n\n\n# Initialize dataset and dataloader\ncsv_file = '/kaggle/input/data/train_solution_bounding_boxes (1).csv'\nimg_dir = '/kaggle/input/data/training_images'\ntransform = transforms.Compose([\n    #data aug \n    transforms.Resize((380, 380)),\n    transforms.ToTensor(),\n])\n\nif DEBUG:\n    print(\"========================================\")\n    print(\"        YOLOv8 Model Debug Output       \")\n    print(\"========================================\")\nmodel = YOLO()\n\ndataset = YOLODataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, batch in enumerate(dataloader):\n        images = batch['image'].to(device)\n        targets = batch['label'].to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        \n        print(outputs.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T14:36:40.513828Z","iopub.execute_input":"2024-08-05T14:36:40.514238Z","iopub.status.idle":"2024-08-05T14:36:41.030419Z","shell.execute_reply.started":"2024-08-05T14:36:40.514208Z","shell.execute_reply":"2024-08-05T14:36:41.028948Z"},"trusted":true},"execution_count":59,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     25\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     29\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 18.12 MiB is free. Process 2334 has 14.72 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 178.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 18.12 MiB is free. Process 2334 has 14.72 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 178.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]}]}