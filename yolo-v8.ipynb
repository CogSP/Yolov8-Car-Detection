{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3866417,"sourceType":"datasetVersion","datasetId":843852},{"sourceId":92363,"sourceType":"modelInstanceVersion","modelInstanceId":77438,"modelId":102070}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":22535.483161,"end_time":"2024-08-10T20:26:34.782076","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-08-10T14:10:59.298915","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reimplementation from scatch of YOLOv8 for Car Detection\n\nThis notebook contains the implementation of the Yolo-v8 architecture. We use it for car detection","metadata":{"papermill":{"duration":0.012822,"end_time":"2024-08-10T14:11:02.149832","exception":false,"start_time":"2024-08-10T14:11:02.137010","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## All the Imports","metadata":{"papermill":{"duration":0.012096,"end_time":"2024-08-10T14:11:02.174088","exception":false,"start_time":"2024-08-10T14:11:02.161992","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image, ImageDraw\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport math","metadata":{"papermill":{"duration":9.19441,"end_time":"2024-08-10T14:11:11.380974","exception":false,"start_time":"2024-08-10T14:11:02.186564","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.845373Z","iopub.execute_input":"2024-08-11T09:58:08.845742Z","iopub.status.idle":"2024-08-11T09:58:08.851973Z","shell.execute_reply.started":"2024-08-11T09:58:08.845710Z","shell.execute_reply":"2024-08-11T09:58:08.851088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Flags Setting\n\n* `TRAINING`: when `True`, running the notebook will train the model and save it.\n\n* `TRAINING_DEBUG`: used to debug the training phase, to understand if the model is learning properly. When both `TRAINING` and `TRAINING_DEBUG` are `False`, running the notebook will test the model.\n\n* `V1_LOSS`: when `True`, a Yolo-v1-like loss is used: we modified the original loss adding a cls loss calculation for grid cell with no-object. If `V1_LOSS` is set to `False`, Yolo-v8 loss is used.","metadata":{"papermill":{"duration":0.013885,"end_time":"2024-08-10T14:11:11.407636","exception":false,"start_time":"2024-08-10T14:11:11.393751","status":"completed"},"tags":[]}},{"cell_type":"code","source":"TRAINING = True\nTRAINING_DEBUG = False\nV1_LOSS = True","metadata":{"papermill":{"duration":0.022512,"end_time":"2024-08-10T14:11:11.443220","exception":false,"start_time":"2024-08-10T14:11:11.420708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.858580Z","iopub.execute_input":"2024-08-11T09:58:08.858891Z","iopub.status.idle":"2024-08-11T09:58:08.866868Z","shell.execute_reply.started":"2024-08-11T09:58:08.858857Z","shell.execute_reply":"2024-08-11T09:58:08.865911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constants\nThe constants above are used to index the encoded target and output in order to retrieve the corresponding quantities.\n\n* `DELTA_X = 0`\n* `DELTA_Y = 1`\n* `WIDTH = 2`\n* `HEIGHT = 3`\n* `CONFIDENCE = 4`\n* `CLASS = 5`","metadata":{"papermill":{"duration":0.012123,"end_time":"2024-08-10T14:11:11.467994","exception":false,"start_time":"2024-08-10T14:11:11.455871","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DELTA_X = 0\nDELTA_Y = 1\nWIDTH = 2\nHEIGHT = 3\nCONFIDENCE = 4\nCLASS = 5","metadata":{"papermill":{"duration":0.020974,"end_time":"2024-08-10T14:11:11.501646","exception":false,"start_time":"2024-08-10T14:11:11.480672","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.868363Z","iopub.execute_input":"2024-08-11T09:58:08.868623Z","iopub.status.idle":"2024-08-11T09:58:08.877803Z","shell.execute_reply.started":"2024-08-11T09:58:08.868601Z","shell.execute_reply":"2024-08-11T09:58:08.876619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils Function\n\n* `scale_to_range`: TODO\n* `show_image_with_boxes`: TODO\n* `from_grid_coordinates_to_bbox`: TODO\n","metadata":{"papermill":{"duration":0.011991,"end_time":"2024-08-10T14:11:11.526041","exception":false,"start_time":"2024-08-10T14:11:11.514050","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def scale_to_range(bboxes, old_x=676, old_y=380, new_x = 128, new_y=128):\n    x_scale = new_x / old_x\n    y_scale = new_y / old_y\n    \n    for box in bboxes:\n        box[0] = int(np.round(box[0]*x_scale))\n        box[1] = int(np.round(box[1]*y_scale))\n        box[2] = int(np.round(box[2]*x_scale))\n        box[3] = int(np.round(box[3]*y_scale))\n    return bboxes","metadata":{"papermill":{"duration":0.022695,"end_time":"2024-08-10T14:11:11.560983","exception":false,"start_time":"2024-08-10T14:11:11.538288","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.881728Z","iopub.execute_input":"2024-08-11T09:58:08.882041Z","iopub.status.idle":"2024-08-11T09:58:08.891981Z","shell.execute_reply.started":"2024-08-11T09:58:08.882008Z","shell.execute_reply":"2024-08-11T09:58:08.890854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WHAT TO DO WITH THE INPUT:\n# STARTING FROM IMAGE_ID AND 4 PARAMETERS FOR BBOX\n# DIVIDE THE IMAGE IN GRID SIZE (128 -> 16, 8 AND 4)\n# CALCULATE THE CENTER OF THE OBJECTS IN THE IMAGE BY CALCULATING THE CENTER OF THE BOUNDING BOX -> WE HAVE IT\n# CALCULATE THE CELL IN WHICH THE CENTER LIES: THAT IS THE CELL RESPONSIBLE OF CALCULATING THE BBOX\n# TRANSFORM THE XYWH QUANTITIES IN DELTA_X DELTA_Y DELTA_W DELTA_H \n# ADD CONFIDENCE 100% AND CLASS PROBABILITY 1 (CAR) FOR THAT GRID\n# PUT ALL ZEROS IN ALL THE OTHER CELLS IN WHICH WE DON'T HAVE OBJECTS\nclass YOLODataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.image_ids = self.annotations['image'].unique()\n        \n        \n    def transform_in_grid_coordinates(self, x_center, y_center, width, height, size=\"large\"):\n        \n        #print(f\"la nostra bounding box è x_center = {x_center}, y_center = {y_center}, width = {width}, height = {height}\\n\")\n    \n        if size == \"small\":\n            number_of_cells = 16\n            n_pixel_per_grid = 8\n        \n        if size == \"medium\":\n            number_of_cells = 8\n            n_pixel_per_grid = 16\n        \n        if size == \"large\":\n            number_of_cells = 4\n            n_pixel_per_grid = 32\n        \n        i = 0\n        for cell in range(number_of_cells * number_of_cells):\n            #print(f\"colonna = {cell}\\n\")\n            #print(f\"riga = {i}\\n\")\n            \n            if cell % number_of_cells  == 0:\n                i += 1\n            \n            x_a = n_pixel_per_grid * (cell % number_of_cells)\n            y_a = n_pixel_per_grid * (i-1) \n            #print(f\"x_a = {x_a}, y_a = {y_a}\\n\")\n            \n            # if verifica se il centro ricade nella cella\n            #print(f\"controlliamo se il centro ricade tra x = {x_a} e x = {x_a + n_pixel_per_grid}...\\n\")\n            if x_center >= x_a and x_center <= (x_a + n_pixel_per_grid):\n                #print(f\"controlliamo se il centro ricade tra x = {y_a} e x = {y_a + n_pixel_per_grid}...\\n\")\n                if y_center >= y_a and y_center <= (y_a + n_pixel_per_grid):\n                    #print(\"il centro ricade qua! Calcoliamo quantità!\\n\")\n                    delta_x = (x_center - x_a) / n_pixel_per_grid\n                    delta_y = (y_center - y_a) / n_pixel_per_grid\n                    delta_width = width / 128\n                    delta_height = height / 128 # 128 is the heigth and width of the image\n                    #print(f\"delta_x = {delta_x}, delta_y = {delta_y}, delta_width = {delta_width}, delta_height = {delta_height}\")\n                    confidence = 1 # 100% confidence that is a car\n                    cl = 1 # 1 = car, 0 = nothing\n                    column = cell\n                    row = i-1\n                    return delta_x, delta_y, delta_width, delta_height, confidence, cl, column%number_of_cells, row\n                else:\n                    continue\n            else:\n                  continue\n            print(\"[ERROR]: there is a box but we have not found the cell in which it lies\")\n            \n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]    \n        \n        # TEMPORARY: This is just an image in which, using grid size 4x4 we have two cars\n        # in the same cell of the grid\n        #image_id = \"vid_4_26420.jpg\"\n        ### REMOVE AFTER DEBUGGING\n        \n        #print(f\"image_id = {image_id}\")\n        \n        img_path = os.path.join(self.img_dir, image_id)\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n                    \n        boxes = self.annotations[self.annotations['image'] == image_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n        #print(f\"ID: {image_id}, shape: {boxes.shape}\\n\")\n        boxes = boxes.astype(float)\n        \n        boxes = scale_to_range(boxes)\n        \n        \n        batch_size = 1 \n        number_of_cells = 4\n        label_encoding = torch.zeros(6, number_of_cells, number_of_cells)\n\n        N_POS = 0\n        for i in boxes:\n            N_POS +=1\n            \n            xmin, ymin, xmax, ymax = i     \n            x_center = int((xmin + xmax) / 2.0)\n            y_center = int((ymin + ymax) / 2.0)\n            width = int(xmax - xmin)\n            height = int(ymax - ymin)\n            delta_x, delta_y, delta_w, delta_h, confidence, cl, column, row = self.transform_in_grid_coordinates(x_center, y_center, width, height, size=\"large\")\n            \n            box = [delta_x, delta_y, delta_w, delta_h, confidence, cl]\n            \n            label_encoding[0][row][column] = box[0]\n            label_encoding[1][row][column] = box[1]\n            label_encoding[2][row][column] = box[2]\n            label_encoding[3][row][column] = box[3]\n            label_encoding[4][row][column] = box[4]\n            label_encoding[5][row][column] = box[5]\n            \n        return image, label_encoding, N_POS\n\n    # DEBUG FUNCTION WHEN TRAINING_DEBUG = True\n    # IT DOES THE SAME THING AS __get_item__ but having \n    # the image_id in input\n    def get_item_debug(self, image_id):\n        img_path = os.path.join(self.img_dir, image_id)\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        boxes = self.annotations[self.annotations['image'] == image_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n        #print(f\"ID: {image_id}, shape: {boxes.shape}\\n\")\n        boxes = boxes.astype(float)\n        \n        boxes = scale_to_range(boxes)\n        \n        batch_size = 1 \n        number_of_cells = 4\n        label_encoding = torch.zeros(6, number_of_cells, number_of_cells)\n\n        N_POS = 0\n        for i in boxes:\n            N_POS +=1\n            \n            xmin, ymin, xmax, ymax = i     \n            x_center = int((xmin + xmax) / 2.0)\n            y_center = int((ymin + ymax) / 2.0)\n            width = int(xmax - xmin)\n            height = int(ymax - ymin)\n            delta_x, delta_y, delta_w, delta_h, confidence, cl, column, row = self.transform_in_grid_coordinates(x_center, y_center, width, height, size=\"large\")\n            \n            box = [delta_x, delta_y, delta_w, delta_h, confidence, cl]\n            \n            label_encoding[0][row][column] = box[0]\n            label_encoding[1][row][column] = box[1]\n            label_encoding[2][row][column] = box[2]\n            label_encoding[3][row][column] = box[3]\n            label_encoding[4][row][column] = box[4]\n            label_encoding[5][row][column] = box[5]\n            \n        return image, label_encoding, N_POS\n","metadata":{"papermill":{"duration":0.045283,"end_time":"2024-08-10T14:11:11.618775","exception":false,"start_time":"2024-08-10T14:11:11.573492","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.894201Z","iopub.execute_input":"2024-08-11T09:58:08.894558Z","iopub.status.idle":"2024-08-11T09:58:08.932299Z","shell.execute_reply.started":"2024-08-11T09:58:08.894523Z","shell.execute_reply":"2024-08-11T09:58:08.931289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image_with_boxes(self, idx):\n        image, boxes_list = self[idx]\n        image_draw = ImageDraw.Draw(image)\n        \n        # Draw the bounding boxes\n        for box in boxes_list:\n            xmin, ymin, xmax, ymax = box\n            image_draw.rectangle([xmin, ymin, xmax, ymax], outline=\"red\", width=2)\n        \n        # Display the image\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"papermill":{"duration":0.024494,"end_time":"2024-08-10T14:11:11.655894","exception":false,"start_time":"2024-08-10T14:11:11.631400","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.948161Z","iopub.execute_input":"2024-08-11T09:58:08.948543Z","iopub.status.idle":"2024-08-11T09:58:08.956014Z","shell.execute_reply.started":"2024-08-11T09:58:08.948507Z","shell.execute_reply":"2024-08-11T09:58:08.954839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Yolo-v8 Architecture","metadata":{"papermill":{"duration":0.012114,"end_time":"2024-08-10T14:11:11.680411","exception":false,"start_time":"2024-08-10T14:11:11.668297","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, k, s, p, c=3, dim=64, mc=512, w=1,flag=1):\n        super(ConvBlock, self).__init__()\n        dim = int(dim)\n        out = min(dim,mc)*w\n        self.conv = nn.Conv2d(in_channels=c, out_channels=out, kernel_size=k, stride=s, padding=p)\n        self.batch_norm = nn.BatchNorm2d(num_features=out)\n        self.activation = nn.SiLU()\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        return x\n    \n\nclass Bottleneck(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, dim=64, shortcut=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = ConvBlock(k,s,p,c,dim=dim,mc=512)\n        self.conv2 = ConvBlock(k,s,p,c,dim=dim,mc=512)\n        self.short = shortcut\n\n    \n    def forward(self, x):\n        #print(x.shape)\n        #print(self.conv1)\n        #print(self.conv2)\n        if self.short: \n            res = self.conv1(x)\n            res = self.conv2(res)\n            return x + res\n        else: \n            res = self.conv1(x)\n            res = self.conv2(x)\n            return res \n            \nclass C2fBlock(nn.Module):\n    def __init__(self, k=1, s=1, p=0, c=3, depth_multiple=1, shortcut=True, dim=64, mc=512, w=1, flag=1):\n        super(C2fBlock, self).__init__() \n        self.conv1 = ConvBlock(k=1,s=1,p=0,c=c,dim=dim,mc=mc,w=w,flag=flag)\n        half_c= int(dim / 2)\n        if half_c == 512:\n            half_dim = 512\n        else:\n            half_dim= int(dim / 2)\n            \n        if flag == 0: \n            self.bottlenecks = nn.ModuleList([Bottleneck(k=3,s=1,p=1,c=256,dim=256) for _ in range(depth_multiple)])\n            new_input = int(512 / 2) * (depth_multiple + 2)\n            self.conv2 = ConvBlock(k,s,p,c=new_input,dim=dim,mc=mc,w=w)\n        else:\n            self.bottlenecks = nn.ModuleList([Bottleneck(k=3,s=1,p=1,c=half_c,dim=half_dim) for _ in range(depth_multiple)])\n            new_input = int(dim / 2) * (depth_multiple + 2)\n            self.conv2 = ConvBlock(k,s,p,c=new_input,dim=dim,mc=mc,w=w)\n    \n    def forward(self, x):\n        \n        #print(f\"x_input: {x.shape}\")\n        x = self.conv1(x)\n              \n       # print(f\"x_conv1: {x.shape}\")\n        \n        # Split the input tensor into two halves along the channel dimension\n        x1, x2 = torch.split(x, x.size(1) // 2, dim=1)\n        \n        #print(f\"x1: {x1.shape}, x2: {x2.shape}\")\n        \n        \n        # Process the other half (x2) through the bottlenecks\n        bottleneck_outputs = []\n        # append half of the input before processing\n        bottleneck_outputs.append(x2.clone())\n        for bott in self.bottlenecks:\n            x2 = bott(x2)\n            bottleneck_outputs.append(x2.clone())\n            \n        # this will concatenate half of the input before processing\n        # and after each bottleneck processing  \n        \n        concatenated_bottleneck_outputs = torch.cat(bottleneck_outputs, dim=1)\n\n        # add the other half\n        x = torch.cat((x1, concatenated_bottleneck_outputs), dim=1)\n        #print(f\"x: {x.shape}, x1: {x1.shape}, conc: {concatenated_bottleneck_outputs.shape}\")\n        x = self.conv2(x)\n        return x\n    \nclass SPPF(nn.Module):\n    def __init__(self, k=3, s=1, p=0, c=3, dim=64):\n        super(SPPF, self).__init__() \n        \n        self.conv1 = ConvBlock(k=k,s=s,p=0,c=c,dim=dim)\n        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n        self.pool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=4)\n        self.pool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=6)\n        self.conv2 = ConvBlock(k=3,s=1,p=1,c=4*c,dim=dim)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        pool1 = self.pool1(x)\n        pool2 = self.pool2(x)\n        pool3 = self.pool3(x)\n        #print(f\"x: {x.shape}, pool1: {pool1.shape}, pool2: {pool2.shape}, pool3: {pool3.shape}\")\n        x = torch.cat([x, pool1, pool2, pool3], dim=1)\n        #print(f\"x_conc: {x.shape}\")\n        x = self.conv2(x)\n        return x\n    \nclass DetectBlock(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, reg_max=1, nc=1, mc=512, w=1):\n        super(DetectBlock, self).__init__()\n        \n        #reg_max = controlla la precisione della regression sulla bounding box \n        #nc = number of classes\n        self.box_conv1 = ConvBlock(k,s,p,c=c,dim=64)\n        self.box_conv2 = ConvBlock(k,s,p,c=64,dim=64)\n        #self.box_conv3 = nn.Conv2d(in_channels=64, out_channels=4*reg_max, kernel_size=1, stride=1, padding=0)\n        # 4 + 1 + 1 = 6 out_channels\n        self.box_conv3 = nn.Conv2d(in_channels=64, out_channels=6, kernel_size=1, stride=1, padding=0)\n        \n        \n        #self.class_conv1 = ConvBlock(k,s,p,c,dim=64)\n        #self.class_conv2 = ConvBlock(k,s,p,c=64,dim=64)\n        #self.class_conv3 = nn.Conv2d(in_channels=64, out_channels=nc, kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x): \n        \n        if DEBUG_2:\n            print(\"[Detect:]\")\n            print(f\"Input: {x.shape}\")\n            print(\"\\t [Conv]\")\n        ret1 = self.box_conv1(x)\n        if DEBUG_2:\n            print(f\"Output: {ret1.shape}\")\n            print(\"\\t [Conv]\")\n        ret1 = self.box_conv2(ret1)\n        if DEBUG_2:\n            print(f\"Output: {ret1.shape}\")\n            print(\"\\t [Conv2D]\")\n            print(self.box_conv3)\n        ret1 = self.box_conv3(ret1)\n        if DEBUG_2:\n            print(f\"Output: {ret1.shape}\")\n        \n        #ret2 = self.class_conv1(x)\n        #ret2 = self.class_conv2(ret2)\n        #ret2 = self.class_conv3(ret2)\n        \n        return ret1#, ret2\n               \nclass BackBone(nn.Module):\n    def __init__(self, k=3, s=2, p=1, depth=1):\n        super(BackBone, self).__init__()\n        \n        self.conv1 = ConvBlock(k,s,p)\n        self.conv2 = ConvBlock(k,s,p, dim=128, c=64)\n        self.c2f = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=128, c=128)\n        self.conv3 = ConvBlock(k,s,p, dim=256, c=128)\n        self.c2f_second = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=256, c=256)\n        self.conv4 = ConvBlock(k,s,p,dim=512, c=256)\n        self.c2f_third = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=512, c=512)\n        self.conv5 = ConvBlock(k,s,p,dim=1024, c=512)\n        self.c2f_last = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=min(1024,512), c=512)\n        \n    def forward(self, x):\n        if DEBUG:\n            print(\"[Layer: Conv 0]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv1(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Conv 1]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv2(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 2]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.c2f(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Conv 3]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")        \n        x = self.conv3(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 4]\")\n            print(f\"Input Tensor Shape:  {x.shape}\") \n        x_first = self.c2f_second(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_first.shape}\")\n\n            print(\"[Layer: Conv 5]\")\n            print(f\"Input Tensor Shape:  {x_first.shape}\") \n        x = self.conv4(x_first)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 6]\")\n            print(f\"Input Tensor Shape:  {x.shape}\") \n        x_second = self.c2f_third(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_second.shape}\")\n\n            print(\"[Layer: Conv 7]\")\n            print(f\"Input Tensor Shape:  {x_second.shape}\")\n        x = self.conv5(x_second)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 8]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x_last = self.c2f_last(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_last.shape}\")\n        \n        return x_first, x_second, x_last\n    \nclass Neck(nn.Module):\n    def __init__(self, depth=1, scale=2):\n        super(Neck, self).__init__()\n        \n        self.sppf = SPPF(k=1,dim=1024,c=512)\n        self.upsample1 = nn.Upsample(scale_factor=2)\n        self.upsample2 = nn.Upsample(scale_factor=2)\n        self.c2f_block1 = C2fBlock(dim=512,c=1024,flag=1,shortcut=False)\n        self.c2f_block2 = C2fBlock(dim=256,c=768,flag=1,shortcut=False) \n        self.c2f_block3 = C2fBlock(dim=512,c=768,flag=1,shortcut=False)\n        self.c2f_block4 = C2fBlock(dim=1024,c=1024,flag=0,shortcut=False)\n        self.conv1 = ConvBlock(k=3,s=2,p=1,dim=256,c=256)\n        self.conv2 = ConvBlock(k=3,s=2,p=1,dim=512,c=512)\n        \n    def forward(self, x_first, x_second, x_last):\n        \n        if DEBUG:\n            print(\"[Layer: SPPF 9]\")\n            print(f\"Input Tensor Shape:  {x_last.shape}\")\n        out_sppf = self.sppf(x_last)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {out_sppf.shape}\")\n\n            print(\"[Layer: Upsample 10]\")\n            print(f\"Input Tensor Shape:  {out_sppf.shape}\")\n        x = self.upsample1(out_sppf)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 11]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {x_second.shape}\")\n        x = torch.cat((x,x_second), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 12]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        conc1 = self.c2f_block1(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {conc1.shape}\")\n\n            print(\"[Layer: Upsample 13]\")\n            print(f\"Input Tensor Shape:  {conc1.shape}\")\n        x = self.upsample2(conc1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 14]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {x_first.shape}\")\n        x = torch.cat((x,x_first), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 15]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det1 = self.c2f_block2(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det1.shape}\")\n\n            print(\"[Layer: Conv 16]\")\n            print(f\"Input Tensor Shape:  {det1.shape}\")\n        x = self.conv1(det1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 17]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {conc1.shape}\")\n        x = torch.cat((x,conc1), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 18]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det2 = self.c2f_block3(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det2.shape}\")\n\n            print(\"[Layer: Conv 19]\")\n            print(f\"Input Tensor Shape:  {det2.shape}\")\n        x = self.conv2(det2)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 20]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {out_sppf.shape}\")\n        x = torch.cat((x,out_sppf), dim=1)\n       \n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 21]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det3 = self.c2f_block4(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det3.shape}\")\n        \n        return det1, det2, det3\n    \nclass Head(nn.Module):\n    def __init__(self):\n        super(Head, self).__init__()\n        \n        #self.det1 = DetectBlock(c=256)\n        #self.det2 = DetectBlock(c=512)\n        self.det3 = DetectBlock(c=512)\n        \n    def forward(self, x3):\n        #return self.det1(x1), self.det2(x2), self.det3(x3)\n        return self.det3(x3)\n    \nclass YOLO(nn.Module):\n    def __init__(self):\n        super(YOLO, self).__init__()\n        self.h1 = BackBone()\n        self.h2 = Neck()\n        self.h3 = Head()\n        \n    def forward(self, x): \n        \n        if DEBUG:\n            print(\"---------- Backbone ----------\")\n            print(\"[Backbone Input]\")\n            print(f\"Input Tensor Shape: {x.shape}\")\n        res1, res2, res3 = self.h1(x)\n        if DEBUG:\n            print(\"[Backbone Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n            print(\"------------------------------\")\n\n        if DEBUG:\n            print(\"---------- Neck ----------\")\n            print(\"[Neck Input]\")\n            print(f\"Input Tensor Shape:  \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n        det1, det2, det3 = self.h2(res1, res2, res3)\n        if DEBUG:\n            print(\"[Neck Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n            print(\"------------------------------\")\n\n        if DEBUG:\n            print(\"---------- Head ----------\")\n            print(\"[Head Input]\")\n            #print(f\"Input Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n            print(f\"Input Tensor Shape: \\n\\t\\t  {det3.shape}\")\n        det3 = self.h3(det3)\n        if DEBUG:\n            print(\"[Head Output]\")\n            #print(f\"Output Tensor Bbox Loss: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n            print(f\"Output Tensor Bbox Loss: \\n\\t\\t     {det3.shape}\")\n            #print(f\"Output Tensor Cls Loss: \\n\\t\\t     {det1[1].shape}, \\n\\t\\t     {det2[1].shape}, \\n\\t\\t     {det3[1].shape}\")\n            print(\"------------------------------\")\n\n        #return det1, det2, det3\n        m = torch.nn.Sigmoid()\n        return m(det3)\n    ","metadata":{"papermill":{"duration":0.084572,"end_time":"2024-08-10T14:11:11.777348","exception":false,"start_time":"2024-08-10T14:11:11.692776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:08.977371Z","iopub.execute_input":"2024-08-11T09:58:08.977615Z","iopub.status.idle":"2024-08-11T09:58:09.046711Z","shell.execute_reply.started":"2024-08-11T09:58:08.977592Z","shell.execute_reply":"2024-08-11T09:58:09.045707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random image from the training-set with its bbox ","metadata":{"papermill":{"duration":0.012223,"end_time":"2024-08-10T14:11:11.802255","exception":false,"start_time":"2024-08-10T14:11:11.790032","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ncsv_file = '/kaggle/input/car-object-detection/data/train_solution_bounding_boxes (1).csv'\n\nimg_dir = '/kaggle/input/car-object-detection/data/training_images'\n\n\n# show a random image from the train-set with its bounding box \ndef show_image(csv_file, img_dir):\n    annotations = pd.read_csv(csv_file)\n \n    img_dir = img_dir\n    image_ids = annotations['image']\n    \n    random_number = random.randint(0, annotations.shape[0] - 1)\n    \n    random_number = 20\n    \n    print(f\"image randomly chosen: {image_ids[random_number]}\")\n    \n    image_id = image_ids[random_number]\n    \n    #img_path = os.path.join(img_dir, image_id)\n    \n    ## JUST FOR TESTING\n    img_path = os.path.join(img_dir, \"vid_4_11280.jpg\")\n    ## REMOVE LATER\n    \n    image = Image.open(img_path).convert(\"RGB\")\n    \n    # Step 3: Convert the tensor to a NumPy array\n    img_numpy = np.array(image)\n    \n    boxes = annotations[annotations['image'] == image_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n    boxes = boxes.astype(float)\n      \n    # Plot the image with bounding boxes\n    fig, ax = plt.subplots(1)\n    ax.imshow(img_numpy)\n    \n    for box in boxes:\n        xmin, ymin, xmax, ymax = box  \n        width = xmax - xmin\n        height = ymax - ymin\n        \n        # Calculate the center point\n        x_center = (xmin + xmax) / 2.0\n        y_center = (ymin + ymax) / 2.0\n        \n        print(f\"bbox = {[x_center, y_center, width, height]}\")\n        \n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        \n        # Plot the center point\n        #ax.plot(x_center, y_center, 'ro')  # 'bo' is for blue circle marker\n    \n        # Add the patch to the Axes\n        ax.add_patch(rect)\n    \n    plt.axis('off')  # Turn off axis\n    plt.show()\n    \nshow_image(csv_file, img_dir)","metadata":{"papermill":{"duration":0.358073,"end_time":"2024-08-10T14:11:12.172602","exception":false,"start_time":"2024-08-10T14:11:11.814529","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.049222Z","iopub.execute_input":"2024-08-11T09:58:09.049626Z","iopub.status.idle":"2024-08-11T09:58:09.294922Z","shell.execute_reply.started":"2024-08-11T09:58:09.049583Z","shell.execute_reply":"2024-08-11T09:58:09.293814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Calculation","metadata":{"papermill":{"duration":0.016045,"end_time":"2024-08-10T14:11:12.204597","exception":false,"start_time":"2024-08-10T14:11:12.188552","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Yolo-v8 Loss Calculation\n\nTODO: put image or latex code","metadata":{"papermill":{"duration":0.015871,"end_time":"2024-08-10T14:11:12.236473","exception":false,"start_time":"2024-08-10T14:11:12.220602","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Yolo-v8 Loss Hyperparameters\n\n* `LAMBDA_BOX`: TODO\n* `LAMBDA_CLS`: TODO","metadata":{"papermill":{"duration":0.01565,"end_time":"2024-08-10T14:11:12.268458","exception":false,"start_time":"2024-08-10T14:11:12.252808","status":"completed"},"tags":[]}},{"cell_type":"code","source":"LAMBDA_BOX = 5.5\nLAMDBA_CLS = 1","metadata":{"papermill":{"duration":0.023865,"end_time":"2024-08-10T14:11:12.308766","exception":false,"start_time":"2024-08-10T14:11:12.284901","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.296551Z","iopub.execute_input":"2024-08-11T09:58:09.296888Z","iopub.status.idle":"2024-08-11T09:58:09.300975Z","shell.execute_reply.started":"2024-08-11T09:58:09.296859Z","shell.execute_reply":"2024-08-11T09:58:09.300058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_IoU(beta_output, beta_target):\n    \n    x_center_output = beta_output[0]\n    y_center_output = beta_output[1]\n    width_output = beta_output[2]\n    heigth_output = beta_output[3]\n\n    x_center_target = beta_target[0]\n    y_center_target = beta_target[1]\n    width_target = beta_target[2]\n    heigth_target = beta_target[3]\n    \n    \n    # first convert from the YOLO format to (x_min, y_min, x_max, y_max)\n    xmin_output = (2*x_center_output - width_output) / 2\n    ymin_output = (2*y_center_output - heigth_output) / 2\n    xmax_output = (2*x_center_output + width_output) / 2 \n    ymax_output = (2*y_center_output + heigth_output) / 2\n\n    boxA = [xmin_output, ymin_output, xmax_output, ymax_output]\n\n    xmin_target = (2*x_center_target - width_target) / 2\n    ymin_target = (2*y_center_target - heigth_target) / 2\n    xmax_target = (2*x_center_target + width_target) / 2 \n    ymax_target = (2*y_center_target + heigth_target) / 2\n\n    boxB = [xmin_target, ymin_target, xmax_target, ymax_target]\n    \n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou\n\n\ndef compute_box_loss(N_POS, indicator, beta_output, beta_target):\n    x_center_output = beta_output[0]\n    y_center_output = beta_output[1]\n    width_output = beta_output[2]\n    heigth_output = beta_output[3]\n\n    x_center_target = beta_target[0]\n    y_center_target = beta_target[1]\n    width_target = beta_target[2]\n    heigth_target = beta_target[3]\n\n    q = compute_IoU(beta_output, beta_target)\n\n    # Compute the L2 norm\n    diff = ((x_center_target - x_center_output),(y_center_target - y_center_output))\n    l2_norm = sum(x**2 for x in diff) ** 0.5\n    \n    #l2_norm = torch.linalg.norm(diff, ord=2)\n\n    # RHO = 1 BECAUSE YES\n    rho = 1\n\n    # I am assuming v_xy is = v in the paper\n    v = (4/math.pi**2) * (math.atan2(width_target, heigth_target) - math.atan2(width_output, heigth_output)) ** 2\n\n    alpha = v / (1 - q)\n\n    box_loss = (LAMBDA_BOX / N_POS) * indicator * ((1 - q) + (l2_norm / rho**2) + (alpha*v))\n    \n    return box_loss\n    \ndef compute_cls_loss(N_POS, y_output, y_target):\n    \n    binary_cross_entropy = y_target*math.log(y_output) + (1 - y_target)*math.log(1 - y_output)\n    \n    # JUST 1 CLASS\n    cls_loss = (LAMBDA_CLS / N_POS) * binary_cross_entropy\n    \n    return cls_loss\n\ndef compute_loss(N_POS, indicator, beta_output, beta_target, y_output, y_target):\n    \n    box_loss = compute_box_loss(N_POS, indicator, beta_output, beta_target)\n    \n    cls_loss = compute_cls_loss(N_POS, y_output, y_target)\n    \n    total_loss = box_loss + cls_loss \n    \n    return total_loss","metadata":{"papermill":{"duration":0.039083,"end_time":"2024-08-10T14:11:12.364631","exception":false,"start_time":"2024-08-10T14:11:12.325548","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.302538Z","iopub.execute_input":"2024-08-11T09:58:09.302924Z","iopub.status.idle":"2024-08-11T09:58:09.327902Z","shell.execute_reply.started":"2024-08-11T09:58:09.302892Z","shell.execute_reply":"2024-08-11T09:58:09.326928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image_and_bbox(image, encoding_of_boxes, filename=None, training=False):\n    \n    image_np = None\n\n    # if a filename is not provided, a 128 x 128 image is displayed\n    if filename == None:\n        image = image[0]\n\n        if image.shape == torch.Size([3, 128, 128]):\n            image_np = image.permute(1, 2, 0).numpy()\n\n    # if a filename is provided, the 676 x 380 image is displayed\n    else:\n        img_path = os.path.join(\"/kaggle/input/car-object-detection/data/testing_images\", filename)\n    \n        if training == True:\n            img_path = os.path.join(\"/kaggle/input/car-object-detection/data/training_images\", filename)\n    \n        image = Image.open(img_path).convert(\"RGB\")\n        image_np = np.array(image)\n            \n    # Plot the image with bounding boxes\n    fig, ax = plt.subplots(1)\n    ax.imshow(image_np)\n    \n    bboxes = from_grid_coordinate_to_bbox(encoding_of_boxes)\n\n    for box in bboxes:\n        \n        #print(f\"box = {box}\")\n\n        x_center = box[0]\n        y_center = box[1]\n        width = box[2]\n        height = box[3]\n        \n        xmin = (2*x_center - width) / 2\n        ymin = (2*y_center - height) / 2\n        xmax = (2*x_center + width) / 2\n        ymax = (2*y_center + height) / 2\n        \n        rect = None\n        # if a filename is not provided, the image is 128 x 128 and so the box should be\n        if filename == None:\n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        \n        # if the image is 676 x 380, the rectangle must be scaled\n        else:\n            bbox = scale_to_range(bboxes=[[xmin, ymin, xmax, ymax]], old_x=128, old_y=128, new_x=676, new_y=380)\n            bbox = bbox[0]\n            xmin = bbox[0]\n            ymin = bbox[1]\n            xmax = bbox[2]\n            ymax = bbox[3]\n            width = xmax - xmin\n            height = ymax - ymin\n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        \n        \n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        \n        \n        confidence_score = box[4]\n        ax.text(xmin, ymin - 5, f'{confidence_score:.4f}', color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n\n    \n    plt.axis('off')  # Turn off axis\n    plt.show()\n    \n    return ","metadata":{"papermill":{"duration":0.033378,"end_time":"2024-08-10T14:11:12.414210","exception":false,"start_time":"2024-08-10T14:11:12.380832","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.330865Z","iopub.execute_input":"2024-08-11T09:58:09.331274Z","iopub.status.idle":"2024-08-11T09:58:09.349226Z","shell.execute_reply.started":"2024-08-11T09:58:09.331236Z","shell.execute_reply":"2024-08-11T09:58:09.348163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def from_grid_coordinate_to_bbox(output, size=\"large\"):\n        \n        if size == \"small\":\n            number_of_cells = 16\n            n_pixel_per_grid = 8\n        \n        if size == \"medium\":\n            number_of_cells = 8\n            n_pixel_per_grid = 16\n        \n        if size == \"large\":\n            number_of_cells = 4\n            n_pixel_per_grid = 32\n    \n        bboxes = []\n        for i in range(number_of_cells):\n            for j in range(number_of_cells):\n                \n                if output[0][CLASS][i][j] >= 0.6:\n                    delta_x = float(output[0][DELTA_X][i][j])\n                    delta_y = float(output[0][DELTA_Y][i][j])\n                    delta_w = float(output[0][WIDTH][i][j])\n                    delta_h = float(output[0][HEIGHT][i][j])\n\n                    x_a = n_pixel_per_grid * j\n                    y_a = n_pixel_per_grid * i\n\n                    x = delta_x * n_pixel_per_grid + x_a\n                    y = delta_y * n_pixel_per_grid + y_a\n                    w = delta_w * 128\n                    h = delta_h * 128\n                    confidence = float(output[0][CONFIDENCE][i][j])                 \n\n                    bbox = [x, y, w, h, confidence]\n                    print(f\"bbox = {bbox}, class = {output[0][CLASS][i][j]}\")\n\n                    bboxes.append(bbox)\n\n        return bboxes                 ","metadata":{"papermill":{"duration":0.031999,"end_time":"2024-08-10T14:11:12.462129","exception":false,"start_time":"2024-08-10T14:11:12.430130","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.350630Z","iopub.execute_input":"2024-08-11T09:58:09.350953Z","iopub.status.idle":"2024-08-11T09:58:09.366410Z","shell.execute_reply.started":"2024-08-11T09:58:09.350925Z","shell.execute_reply":"2024-08-11T09:58:09.365537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Yolo-v1 Loss Calculation\n\nWe have modified the classic Yolo-v1 loss, adding a cls loss calculation term also for no-object cell. Given position $(x,y)$ for each cell of the grid, we have that the loss is: \n\n$$\n\\displaystyle\\sum_{x,y} (\\mathcal{L}_{obj} + \\lambda_{no-obj}\\mathcal{L}_{no-obj})\n$$\n\nWhere the loss function used when a cell contains an object is:\n$$\n\\mathcal{L}_{obj} = \\lambda_{coord}\\mathcal{L}_{obj-box} + \\mathcal{L}_{obj-conf} + \\mathcal{L}_{obj-class} \n$$\n\nWith: \n\n$$\n\\mathcal{L}_{obj-box} = (\\hat{\\Delta{x}} - \\Delta{x})^2 + (\\hat{\\Delta{y}} - \\Delta{y})^2 + (\\sqrt{\\hat{\\Delta{w}}} - \\sqrt{\\Delta{w}})^2 + (\\sqrt{\\hat{\\Delta{h}}} - \\sqrt{\\Delta{h}})^2\n$$\n\n$$\n\\mathcal{L}_{obj-conf} = (\\hat{{C}} - {C})^2 \n$$\n\n$$\n\\mathcal{L}_{obj-cls} = (\\hat{p} - {p})^2 \n$$\nwith $C = 1$ and $p = 1$\n\nWhile the loss function used when a cell does not contain an object is:\n$$\n\\mathcal{L}_{obj} = \\mathcal{L}_{no-obj-conf} + \\mathcal{L}_{no-obj-class}\n$$\nWith analogous expression for the class and confidence losses, with the only difference of $C = 0$ and $p = 0$","metadata":{"papermill":{"duration":0.017257,"end_time":"2024-08-10T14:11:12.495842","exception":false,"start_time":"2024-08-10T14:11:12.478585","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Yolo-v1 Loss Hyperparameters\n\n* `LAMBDA_COORD`: when > 1 it puts more importance on box parameters than cls and confidence\n* `LAMBDA_NO_OBJ`: when < 1 it puts more importance on grid cells that contain object than cells which don't","metadata":{"papermill":{"duration":0.015704,"end_time":"2024-08-10T14:11:12.527851","exception":false,"start_time":"2024-08-10T14:11:12.512147","status":"completed"},"tags":[]}},{"cell_type":"code","source":"LAMBDA_COORD = 5\nLAMBDA_NO_OBJ = 1","metadata":{"papermill":{"duration":0.02436,"end_time":"2024-08-10T14:11:12.568274","exception":false,"start_time":"2024-08-10T14:11:12.543914","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.368388Z","iopub.execute_input":"2024-08-11T09:58:09.369004Z","iopub.status.idle":"2024-08-11T09:58:09.383871Z","shell.execute_reply.started":"2024-08-11T09:58:09.368971Z","shell.execute_reply":"2024-08-11T09:58:09.382957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_V1_loss(target, output, obj_indicator, no_obj_indicator): \n\n    #print(f\"\\t[TARGET:] = \\n \\t delta_x = {[target[0][DELTA_X][i][j]]} \\n \\t delta_y = {[target[0][DELTA_Y][i][j]]} \\n \\t delta_w = {[target[0][WIDTH][i][j]]} \\n \\t delta_h = {[target[0][HEIGHT][i][j]]} \\n \\t confidence = {[target[0][CONFIDENCE][i][j]]} \\n \\t class = {[target[0][CLASS][i][j]]} \")\n    #print(f\"\\t[OUTPUT:] = \\n \\t delta_x = {[output[0][DELTA_X][i][j]]} \\n \\t delta_y = {[output[0][DELTA_Y][i][j]]} \\n \\t delta_w = {[output[0][WIDTH][i][j]]} \\n \\t delta_h = {[output[0][HEIGHT][i][j]]} \\n \\t confidence = {[output[0][CONFIDENCE][i][j]]} \\n \\t class = {[output[0][CLASS][i][j]]} \\n\")\n    # obj loss calculation\n    #print(f\"\\t OBJ LOSS:\\n\")\n    \n    loss_box_obj_DELTA_X = (target[0][DELTA_X][i][j] - output[0][DELTA_X][i][j])**2\n    loss_box_obj_DELTA_Y = (target[0][DELTA_Y][i][j] - output[0][DELTA_Y][i][j])**2\n    loss_box_obj_WIDTH = (torch.sqrt(target[0][WIDTH][i][j]) - torch.sqrt(output[0][WIDTH][i][j]))**2\n    loss_box_obj_HEIGHT = (torch.sqrt(target[0][HEIGHT][i][j]) - torch.sqrt(output[0][HEIGHT][i][j]))**2\n    loss_box_obj = loss_box_obj_DELTA_X + loss_box_obj_DELTA_Y + loss_box_obj_WIDTH + loss_box_obj_HEIGHT\n    \n    #print(f\"\\t \\t box loss delta_x = {loss_box_obj_DELTA_X}\")\n    #print(f\"\\t \\t box loss delta_y = {loss_box_obj_DELTA_Y}\")\n    #print(f\"\\t \\t box loss delta_w = {loss_box_obj_WIDTH}\")\n    #print(f\"\\t \\t box loss delta_h = {loss_box_obj_HEIGHT}\")\n    #print(f\"\\t \\t total box loss = {loss_box_obj}\\n\")\n    \n    loss_conf_obj = (target[0][CONFIDENCE][i][j] - output[0][CONFIDENCE][i][j])**2\n    #print(f\"loss_conf_obj = {loss_conf_obj}\")\n    \n    #print(f\"\\t \\t conf loss = {loss_conf_obj}\\n\")\n\n    loss_cls_obj = (target[0][CLASS][i][j] - output[0][CLASS][i][j])**2\n    #print(f\"loss_cls_obj = {loss_cls_obj}\")\n    \n    #print(f\"\\t \\t cls loss = {loss_cls_obj}\\n\")\n\n    cell_loss_obj = obj_indicator*((LAMBDA_COORD*loss_box_obj) + loss_conf_obj + loss_cls_obj)\n    #print(f\"cell_loss_obj = {cell_loss_obj}\")\n\n    #print(f\"\\t \\t TOTAL OBJ LOSS = {cell_loss_obj}\\n\")\n\n    #print(f\"\\t NO OBJ LOSS:\\n\")\n    \n    # no obj loss calculation \n    loss_conf_no_obj = (target[0][CONFIDENCE][i][j] - output[0][CONFIDENCE][i][j])**2\n    #print(f\"loss_conf_no_obj = {loss_conf_no_obj}\")\n    loss_cls_no_obj = (target[0][CLASS][i][j] - output[0][CLASS][i][j])**2\n    #print(f\"loss_cls_no_obj = {loss_cls_no_obj}\")\n    \n    #print(f\"\\t \\t conf loss = {loss_conf_no_obj}\\n\")\n\n    cell_loss_no_obj = LAMBDA_NO_OBJ * no_obj_indicator * (loss_conf_no_obj + loss_cls_no_obj)\n    #print(f\"cell_loss_no_obj = {cell_loss_no_obj}\")\n\n    #print(f\"\\t \\t TOTAL NO OBJ LOSS = {cell_loss_no_obj}\\n\")\n\n    total_loss = cell_loss_obj + cell_loss_no_obj\n    #print(f\"cell_loss_obj = {cell_loss_obj} cell_loss_no_obj = {cell_loss_no_obj}\")\n    #print(f\"total_loss = {total_loss}\\n\\n\\n\\n\")\n    \n    #print(f\"\\t TOTAL LOSS OF THE CELL = {total_loss}\")\n    \n    return total_loss\n","metadata":{"papermill":{"duration":0.032766,"end_time":"2024-08-10T14:11:12.617309","exception":false,"start_time":"2024-08-10T14:11:12.584543","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.385154Z","iopub.execute_input":"2024-08-11T09:58:09.385496Z","iopub.status.idle":"2024-08-11T09:58:09.399386Z","shell.execute_reply.started":"2024-08-11T09:58:09.385469Z","shell.execute_reply":"2024-08-11T09:58:09.398458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Phase","metadata":{"papermill":{"duration":0.016075,"end_time":"2024-08-10T14:11:12.649592","exception":false,"start_time":"2024-08-10T14:11:12.633517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# DEBUGGING OPTIONS\n\n# FIRST LEVEL OF GRANULARITY: IT SHOWS INPUT AND OUTPUT SIZES OF THE MAIN BLOCK OF THE ARCHITECTURE\nDEBUG = False\n\n# SECOND LEVEL OF GRANULARITY: IT SHOWS INPUT AND OUTPUT SIZES OF INTERNAL SUB-BLOCK TOO\nDEBUG_2 = False\n\nif TRAINING == True:\n\n    # Initialize dataset and dataloader\n    \n    num_epochs = 700\n\n    csv_file = '/kaggle/input/car-object-detection/data/train_solution_bounding_boxes (1).csv'\n    img_dir = '/kaggle/input/car-object-detection/data/training_images'\n    transform = transforms.Compose([\n        #data aug \n        # TRY MOSAIC AUGMENTATION\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n\n    if DEBUG:\n        print(\"========================================\")\n        print(\"        YOLOv8 Model Debug Output       \")\n        print(\"========================================\")\n    model = YOLO()\n\n    dataset = YOLODataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\n    #dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=1)\n\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    model.to(device)\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        for image, target, N_POS in dataloader:\n            \n            # just for debugging\n            #andand_bbox(image, target)\n\n            target = target.to(device)\n            image = image.to(device)\n\n            optimizer.zero_grad()\n\n            # Predizioni del modello\n            output = model(image)\n\n            n_row = target.shape[2]\n            n_column = target.shape[3]\n\n            \n            total_loss = 0\n            for i in range(0, n_row):\n                for j in range(0, n_column):\n\n                    # computing indicators for obj and no_obj loss\n                    if target[0][CONFIDENCE][i][j] == 0: # -> there is no object!\n                        obj_indicator = 0\n                        no_obj_indicator = 1\n                    else:\n                        obj_indicator = 1\n                        no_obj_indicator = 0\n\n                    if V1_LOSS == True:\n                        total_loss += compute_V1_loss(target, output, obj_indicator, no_obj_indicator)\n                    else:\n                        beta_output = output[0][:4][i][j]\n                        beta_target = target[0][:4][i][j]\n                        y_output = output[0][5][i][j]\n                        y_target = target[0][5][i][j]\n                        total_loss += compute_V8_loss(N_POS, obj_indicator, beta_output, beta_target, y_output, y_target)\n\n            total_loss.backward()\n            optimizer.step()\n\n            epoch_loss += total_loss.item()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")","metadata":{"papermill":{"duration":22516.271159,"end_time":"2024-08-10T20:26:28.937103","exception":false,"start_time":"2024-08-10T14:11:12.665944","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:09.400640Z","iopub.execute_input":"2024-08-11T09:58:09.401008Z","iopub.status.idle":"2024-08-11T09:58:20.060567Z","shell.execute_reply.started":"2024-08-11T09:58:09.400980Z","shell.execute_reply":"2024-08-11T09:58:20.058878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Debug Training","metadata":{"papermill":{"duration":0.073907,"end_time":"2024-08-10T20:26:29.084754","exception":false,"start_time":"2024-08-10T20:26:29.010847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if TRAINING_DEBUG == True:    \n    \n    # Initialize dataset and dataloader\n\n    num_epochs = 20\n\n    csv_file = '/kaggle/input/car-object-detection/data/train_solution_bounding_boxes (1).csv'\n    img_dir = '/kaggle/input/car-object-detection/data/training_images'\n    transform = transforms.Compose([\n        #data aug \n        # TRY MOSAIC AUGMENTATION\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n\n    if DEBUG:\n        print(\"========================================\")\n        print(\"        YOLOv8 Model Debug Output       \")\n        print(\"========================================\")\n    model = YOLO()\n\n    dataset = YOLODataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\n    \n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    model.to(device)\n\n    print(\"STARTING....\\n\")\n    for epoch in range(num_epochs):\n        \n        model.train()\n        epoch_loss = 0.0\n        \n        # prendi l'immagine \n        image, target, N_POS = dataset.get_item_debug(\"vid_4_28220.jpg\")\n        \n        # to add the batch_size dimension as the dataloader does\n        image = image.unsqueeze(0)\n        target = target.unsqueeze(0)\n        \n        if epoch == 0:\n            #just for debugging\n            show_image_and_bbox(image, target)\n            print(f\"[TARGET:] = \\n {target}\\n\")\n\n        target = target.to(device)\n        image = image.to(device)\n        \n\n        optimizer.zero_grad()\n\n        # Predizioni del modello\n        output = model(image)\n        \n        print(f\"[OUTPUT:] = \\n {output}\\n\")\n        \n        n_row = target.shape[2]\n        n_column = target.shape[3]\n\n\n        print(\"CALCULATE LOSS BETWEEN [TARGET] AND [OUTPUT]\\n\")\n        total_loss = 0\n        for i in range(0, n_row):\n            for j in range(0, n_column):\n                print(f\"\\tGrid Cell: row {i} column {j}\\n\")\n\n                # computing indicators for obj and no_obj loss\n                if target[0][CONFIDENCE][i][j] == 0: # -> there is no object!\n                    obj_indicator = 0\n                    no_obj_indicator = 1\n                else:\n                    obj_indicator = 1\n                    no_obj_indicator = 0\n\n                if V1_LOSS == True:\n                    total_loss += compute_V1_loss(target, output, obj_indicator, no_obj_indicator)\n                    print(\"\\n\")\n                else:\n                    beta_output = output[0][:4][i][j]\n                    beta_target = target[0][:4][i][j]\n                    y_output = output[0][5][i][j]\n                    y_target = target[0][5][i][j]\n                    total_loss += compute_V8_loss(N_POS, obj_indicator, beta_output, beta_target, y_output, y_target)\n\n        total_loss.backward()\n        optimizer.step()\n\n        epoch_loss += total_loss.item()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / 1}\")\n        show_image_and_bbox(image.cpu(), output)\n        print(\"\\n\")","metadata":{"papermill":{"duration":0.104495,"end_time":"2024-08-10T20:26:29.266349","exception":false,"start_time":"2024-08-10T20:26:29.161854","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.061961Z","iopub.status.idle":"2024-08-11T09:58:20.062461Z","shell.execute_reply.started":"2024-08-11T09:58:20.062215Z","shell.execute_reply":"2024-08-11T09:58:20.062235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the Model after Training","metadata":{"papermill":{"duration":0.074043,"end_time":"2024-08-10T20:26:29.414041","exception":false,"start_time":"2024-08-10T20:26:29.339998","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if TRAINING == True and TRAINING_DEBUG == False:\n    torch.save(model.state_dict(), '/kaggle/working/yolov8_new_700_epoch.pth')\n    print(\"model saved\")","metadata":{"papermill":{"duration":0.384421,"end_time":"2024-08-10T20:26:29.872215","exception":false,"start_time":"2024-08-10T20:26:29.487794","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.064229Z","iopub.status.idle":"2024-08-11T09:58:20.064709Z","shell.execute_reply.started":"2024-08-11T09:58:20.064436Z","shell.execute_reply":"2024-08-11T09:58:20.064454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This section is only for train debugging\nif TRAINING_DEBUG == True and TRAINING == False:\n    !mkdir /kaggle/working/debugging\n    torch.save(model.state_dict(), '/kaggle/working/debugging/yolov8_new_100_epoch_DEBUGGING.pht')\n    print(\"debug model saved\")","metadata":{"papermill":{"duration":0.083431,"end_time":"2024-08-10T20:26:30.030240","exception":false,"start_time":"2024-08-10T20:26:29.946809","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.066905Z","iopub.status.idle":"2024-08-11T09:58:20.067415Z","shell.execute_reply.started":"2024-08-11T09:58:20.067173Z","shell.execute_reply":"2024-08-11T09:58:20.067193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{"papermill":{"duration":0.074855,"end_time":"2024-08-10T20:26:30.180981","exception":false,"start_time":"2024-08-10T20:26:30.106126","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Load the Model ","metadata":{"papermill":{"duration":0.073862,"end_time":"2024-08-10T20:26:30.328688","exception":false,"start_time":"2024-08-10T20:26:30.254826","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if TRAINING == False and TRAINING_DEBUG == False:\n    model = YOLO();\n    model_load_path = '/kaggle/input/yolo_v8_100_epoch/pytorch/default/1/yolov8_new_100_epoch.pth'\n    model.load_state_dict(torch.load(model_load_path));\n    model.to(device);\n    model.eval();  # Set the model to evaluation mode\n    print(\"model loaded\")","metadata":{"papermill":{"duration":0.082913,"end_time":"2024-08-10T20:26:30.484982","exception":false,"start_time":"2024-08-10T20:26:30.402069","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.068496Z","iopub.status.idle":"2024-08-11T09:58:20.068979Z","shell.execute_reply.started":"2024-08-11T09:58:20.068732Z","shell.execute_reply":"2024-08-11T09:58:20.068753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test the Model","metadata":{"papermill":{"duration":0.073574,"end_time":"2024-08-10T20:26:30.632033","exception":false,"start_time":"2024-08-10T20:26:30.558459","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if TRAINING == False and TRAINING_DEBUG == False:\n\n    # Testing \n    test_transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n\n    test_img_dir = '/kaggle/input/car-object-detection/data/testing_images'\n    file_list = os.listdir(test_img_dir)\n    TEST_SET_SIZE = len(file_list)\n\n    random_number = random.randint(0, TEST_SET_SIZE)\n\n    filename = None\n    \n    image = None\n    for i in range(TEST_SET_SIZE):\n        if i == random_number:\n            filename = file_list[i]\n            print(f\"filename = {filename} randomly chosen\")\n\n            img_path = os.path.join(test_img_dir, filename)\n\n            image = Image.open(img_path).convert(\"RGB\")\n\n            break \n            \n\n    image = test_transform(image)\n    image = image.unsqueeze(0)\n    image = image.to(device)\n\n    model.eval()\n\n    with torch.no_grad():\n        \n        out = model(image)\n        \n        # giving filename to the function, it displays the result in 676 x 380\n        # without it, it displays it in 128 x 128\n        \n        # if training = true you want to use images from the training set\n        show_image_and_bbox(image.cpu(), out, filename, training=False)","metadata":{"papermill":{"duration":0.089368,"end_time":"2024-08-10T20:26:30.797400","exception":false,"start_time":"2024-08-10T20:26:30.708032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.070834Z","iopub.status.idle":"2024-08-11T09:58:20.071306Z","shell.execute_reply.started":"2024-08-11T09:58:20.071055Z","shell.execute_reply":"2024-08-11T09:58:20.071074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Processing","metadata":{"papermill":{"duration":0.074176,"end_time":"2024-08-10T20:26:30.947101","exception":false,"start_time":"2024-08-10T20:26:30.872925","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Put the result in a directory for further processing","metadata":{"papermill":{"duration":0.072857,"end_time":"2024-08-10T20:26:31.093298","exception":false,"start_time":"2024-08-10T20:26:31.020441","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Put jpg in kaggle/working/results","metadata":{"papermill":{"duration":0.081929,"end_time":"2024-08-10T20:26:31.248073","exception":false,"start_time":"2024-08-10T20:26:31.166144","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.072847Z","iopub.status.idle":"2024-08-11T09:58:20.073181Z","shell.execute_reply.started":"2024-08-11T09:58:20.073019Z","shell.execute_reply":"2024-08-11T09:58:20.073033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From a sequence of JPG to a video ","metadata":{"papermill":{"duration":0.07688,"end_time":"2024-08-10T20:26:31.398985","exception":false,"start_time":"2024-08-10T20:26:31.322105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\n\ndef create_video_from_images(image_folder, output_video_path, frame_rate=30):\n    # Ottieni una lista di tutti i file nella cartella delle immagini\n    images = [img for img in os.listdir(image_folder) if img.endswith((\".jpg\"))]\n    \n    # Se la lista delle immagini è vuota, esci\n    if not images:\n        print(\"La cartella delle immagini è vuota.\")\n        return\n    \n    # Leggi la prima immagine per ottenere le dimensioni del video\n    first_image_path = os.path.join(image_folder, images[0])\n    frame = cv2.imread(first_image_path)\n    height, width, layers = frame.shape\n\n    # Definisci il codec e crea l'oggetto VideoWriter\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n    \n    # Aggiungi tutte le immagini al video\n    for image in images:\n        image_path = os.path.join(image_folder, image)\n        frame = cv2.imread(image_path)\n        video.write(frame)\n    \n    # Rilascia l'oggetto VideoWriter\n    video.release()\n    print(f\"Video creato con successo: {output_video_path}\")\n\n# Esempio di utilizzo\nimage_folder = '/kaggle/input/car-object-detection/data/testing_images'\noutput_video_path = '/kaggle/working/video.mp4'\ncreate_video_from_images(image_folder, output_video_path, frame_rate=30)\n\n\n\"\"\"","metadata":{"papermill":{"duration":0.085108,"end_time":"2024-08-10T20:26:31.558963","exception":false,"start_time":"2024-08-10T20:26:31.473855","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.074460Z","iopub.status.idle":"2024-08-11T09:58:20.074856Z","shell.execute_reply.started":"2024-08-11T09:58:20.074654Z","shell.execute_reply":"2024-08-11T09:58:20.074695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From Video to sequence of JPG","metadata":{"papermill":{"duration":0.073603,"end_time":"2024-08-10T20:26:31.705591","exception":false,"start_time":"2024-08-10T20:26:31.631988","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\n\ndef create_images_from_video(video_path, output_dir, ):\n\n    # Create the directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n\n    frame_count = 0\n\n    while True:\n        # Read a frame from the video\n        ret, frame = cap.read()\n\n        # If no frame is read (end of the video), break the loop\n        if not ret:\n            break\n\n        # Construct the file name for the image\n        image_path = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n\n        # Save the frame as a JPEG file\n        cv2.imwrite(image_path, frame)\n\n        # Increment the frame count\n        frame_count += 1\n\n    # Release the video capture object\n    cap.release()\n\n    print(f'Extracted {frame_count} frames and saved to {output_dir}')\n    \n    \n\"\"\"","metadata":{"papermill":{"duration":0.087406,"end_time":"2024-08-10T20:26:31.867480","exception":false,"start_time":"2024-08-10T20:26:31.780074","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-08-11T09:58:20.076412Z","iopub.status.idle":"2024-08-11T09:58:20.076774Z","shell.execute_reply.started":"2024-08-11T09:58:20.076576Z","shell.execute_reply":"2024-08-11T09:58:20.076590Z"},"trusted":true},"execution_count":null,"outputs":[]}]}