{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3866417,"sourceType":"datasetVersion","datasetId":843852}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:28:43.812977Z","iopub.execute_input":"2024-08-07T06:28:43.813329Z","iopub.status.idle":"2024-08-07T06:28:43.819339Z","shell.execute_reply.started":"2024-08-07T06:28:43.813301Z","shell.execute_reply":"2024-08-07T06:28:43.818328Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def scale_to_range(array, new_min=0, new_max=380):\n    old_min = np.min(array)\n    old_max = np.max(array)\n    \n    scaled_array = new_min + ((array - old_min) * (new_max - new_min) / (old_max - old_min))\n    return scaled_array\n\nclass YOLODataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.image_ids = self.annotations['image'].unique()\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        img_path = os.path.join(self.img_dir, image_id)\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        boxes = self.annotations[self.annotations['image'] == image_id][['xmin', 'ymin', 'xmax', 'ymax']].values\n        #print(f\"ID: {image_id}, shape: {boxes.shape}\\n\")\n        boxes = boxes.astype(float)\n        \n        boxes = scale_to_range(boxes)\n        \n        boxes_list = []\n        for i in boxes:\n            xmin, ymin, xmax, ymax = i\n            x_center = (xmin + xmax) / 2.0\n            y_center = (ymin + ymax) / 2.0\n            width = xmax - xmin\n            height = ymax - ymin\n            box = np.array([x_center, y_center, width, height], dtype=np.float32)\n            \n            # Ensure box has shape [1, 4]\n            if box.ndim == 1:\n                box = box.reshape(1, 4)\n            boxes_list.append(box)\n        \n        if len(boxes_list) > 0:\n            boxes_tens = torch.tensor(np.concatenate(boxes_list, axis=0), dtype=torch.float32)\n        else:\n            boxes_tens = torch.empty((0, 4), dtype=torch.float32)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        target = {\n            'boxes': boxes_tens,\n            'labels': torch.ones((boxes_tens.shape[0],), dtype=torch.int64)  # Supponiamo che ci sia solo una classe\n        }\n        \n        return image, target","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:28:43.839642Z","iopub.execute_input":"2024-08-07T06:28:43.839924Z","iopub.status.idle":"2024-08-07T06:28:43.852422Z","shell.execute_reply.started":"2024-08-07T06:28:43.839901Z","shell.execute_reply":"2024-08-07T06:28:43.851529Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, k, s, p, c=3, dim=64, mc=512, w=1,flag=1):\n        super(ConvBlock, self).__init__()\n        dim = int(dim)\n        out = min(dim,mc)*w\n        self.conv = nn.Conv2d(in_channels=c, out_channels=out, kernel_size=k, stride=s, padding=p)\n        self.batch_norm = nn.BatchNorm2d(num_features=out)\n        self.activation = nn.SiLU()\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        return x\n    \n\nclass Bottleneck(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, dim=64, shortcut=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = ConvBlock(k,s,p,c,dim=dim,mc=512)\n        self.conv2 = ConvBlock(k,s,p,c,dim=dim,mc=512)\n        self.short = shortcut\n\n    \n    def forward(self, x):\n        #print(x.shape)\n        #print(self.conv1)\n        #print(self.conv2)\n        if self.short: \n            res = self.conv1(x)\n            res = self.conv2(res)\n            return x + res\n        else: \n            res = self.conv1(x)\n            res = self.conv2(x)\n            return res \n            \nclass C2fBlock(nn.Module):\n    def __init__(self, k=1, s=1, p=0, c=3, depth_multiple=1, shortcut=True, dim=64, mc=512, w=1, flag=1):\n        super(C2fBlock, self).__init__() \n        self.conv1 = ConvBlock(k=1,s=1,p=0,c=c,dim=dim,mc=mc,w=w,flag=flag)\n        half_c= int(dim / 2)\n        if half_c == 512:\n            half_dim = 512\n        else:\n            half_dim= int(dim / 2)\n            \n        if flag == 0: \n            self.bottlenecks = nn.ModuleList([Bottleneck(k=3,s=1,p=1,c=256,dim=256) for _ in range(depth_multiple)])\n            new_input = int(512 / 2) * (depth_multiple + 2)\n            self.conv2 = ConvBlock(k,s,p,c=new_input,dim=dim,mc=mc,w=w)\n        else:\n            self.bottlenecks = nn.ModuleList([Bottleneck(k=3,s=1,p=1,c=half_c,dim=half_dim) for _ in range(depth_multiple)])\n            new_input = int(dim / 2) * (depth_multiple + 2)\n            self.conv2 = ConvBlock(k,s,p,c=new_input,dim=dim,mc=mc,w=w)\n    \n    def forward(self, x):\n        \n        #print(f\"x_input: {x.shape}\")\n        x = self.conv1(x)\n              \n       # print(f\"x_conv1: {x.shape}\")\n        \n        # Split the input tensor into two halves along the channel dimension\n        x1, x2 = torch.split(x, x.size(1) // 2, dim=1)\n        \n        #print(f\"x1: {x1.shape}, x2: {x2.shape}\")\n        \n        \n        # Process the other half (x2) through the bottlenecks\n        bottleneck_outputs = []\n        # append half of the input before processing\n        bottleneck_outputs.append(x2.clone())\n        for bott in self.bottlenecks:\n            x2 = bott(x2)\n            bottleneck_outputs.append(x2.clone())\n            \n        # this will concatenate half of the input before processing\n        # and after each bottleneck processing  \n        \n        concatenated_bottleneck_outputs = torch.cat(bottleneck_outputs, dim=1)\n\n        # add the other half\n        x = torch.cat((x1, concatenated_bottleneck_outputs), dim=1)\n        #print(f\"x: {x.shape}, x1: {x1.shape}, conc: {concatenated_bottleneck_outputs.shape}\")\n        x = self.conv2(x)\n        return x\n    \nclass SPPF(nn.Module):\n    def __init__(self, k=3, s=1, p=0, c=3, dim=64):\n        super(SPPF, self).__init__() \n        \n        self.conv1 = ConvBlock(k=k,s=s,p=0,c=c,dim=dim)\n        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n        self.pool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=4)\n        self.pool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=6)\n        self.conv2 = ConvBlock(k=3,s=1,p=1,c=4*c,dim=dim)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        pool1 = self.pool1(x)\n        pool2 = self.pool2(x)\n        pool3 = self.pool3(x)\n        #print(f\"x: {x.shape}, pool1: {pool1.shape}, pool2: {pool2.shape}, pool3: {pool3.shape}\")\n        x = torch.cat([x, pool1, pool2, pool3], dim=1)\n        #print(f\"x_conc: {x.shape}\")\n        x = self.conv2(x)\n        return x\n    \nclass DetectBlock(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, reg_max=1, nc=1, mc=512, w=1):\n        super(DetectBlock, self).__init__()\n        \n        #reg_max = controlla la precisione della regression sulla bounding box \n        #nc = number of classes\n        self.box_conv1 = ConvBlock(k,s,p,c=c,dim=64)\n        self.box_conv2 = ConvBlock(k,s,p,c=64,dim=64)\n        self.box_conv3 = nn.Conv2d(in_channels=64, out_channels=4*reg_max, kernel_size=1, stride=1, padding=0)\n        \n        self.class_conv1 = ConvBlock(k,s,p,c,dim=64)\n        self.class_conv2 = ConvBlock(k,s,p,c=64,dim=64)\n        self.class_conv3 = nn.Conv2d(in_channels=64, out_channels=nc, kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x): \n        print(\"[Detect:]\")\n        print(f\"Input: {x.shape}\")\n        print(\"\\t [Conv]\")\n        ret1 = self.box_conv1(x)\n        print(f\"Output: {ret1.shape}\")\n        print(\"\\t [Conv]\")\n        ret1 = self.box_conv2(ret1)\n        print(f\"Output: {ret1.shape}\")\n        print(\"\\t [Conv2D]\")\n        print(self.box_conv3)\n        ret1 = self.box_conv3(ret1)\n        print(f\"Output: {ret1.shape}\")\n        \n        ret2 = self.class_conv1(x)\n        ret2 = self.class_conv2(ret2)\n        ret2 = self.class_conv3(ret2)\n        \n        return ret1, ret2\n               \nclass BackBone(nn.Module):\n    def __init__(self, k=3, s=2, p=1, depth=1):\n        super(BackBone, self).__init__()\n        \n        self.conv1 = ConvBlock(k,s,p)\n        self.conv2 = ConvBlock(k,s,p, dim=128, c=64)\n        self.c2f = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=128, c=128)\n        self.conv3 = ConvBlock(k,s,p, dim=256, c=128)\n        self.c2f_second = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=256, c=256)\n        self.conv4 = ConvBlock(k,s,p,dim=512, c=256)\n        self.c2f_third = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=512, c=512)\n        self.conv5 = ConvBlock(k,s,p,dim=1024, c=512)\n        self.c2f_last = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=min(1024,512), c=512)\n        \n    def forward(self, x):\n        if DEBUG:\n            print(\"[Layer: Conv 0]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv1(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Conv 1]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv2(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 2]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.c2f(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Conv 3]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")        \n        x = self.conv3(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 4]\")\n            print(f\"Input Tensor Shape:  {x.shape}\") \n        x_first = self.c2f_second(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_first.shape}\")\n\n            print(\"[Layer: Conv 5]\")\n            print(f\"Input Tensor Shape:  {x_first.shape}\") \n        x = self.conv4(x_first)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 6]\")\n            print(f\"Input Tensor Shape:  {x.shape}\") \n        x_second = self.c2f_third(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_second.shape}\")\n\n            print(\"[Layer: Conv 7]\")\n            print(f\"Input Tensor Shape:  {x_second.shape}\")\n        x = self.conv5(x_second)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 8]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        x_last = self.c2f_last(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x_last.shape}\")\n        \n        return x_first, x_second, x_last\n    \nclass Neck(nn.Module):\n    def __init__(self, depth=1, scale=2):\n        super(Neck, self).__init__()\n        \n        self.sppf = SPPF(k=1,dim=1024,c=512)\n        self.upsample1 = nn.Upsample(scale_factor=2)\n        self.upsample2 = nn.Upsample(scale_factor=2)\n        self.c2f_block1 = C2fBlock(dim=512,c=1024,flag=1,shortcut=False)\n        self.c2f_block2 = C2fBlock(dim=256,c=768,flag=1,shortcut=False) \n        self.c2f_block3 = C2fBlock(dim=512,c=768,flag=1,shortcut=False)\n        self.c2f_block4 = C2fBlock(dim=1024,c=1024,flag=0,shortcut=False)\n        self.conv1 = ConvBlock(k=3,s=2,p=1,dim=256,c=256)\n        self.conv2 = ConvBlock(k=3,s=2,p=1,dim=512,c=512)\n        \n    def forward(self, x_first, x_second, x_last):\n        \n        if DEBUG:\n            print(\"[Layer: SPPF 9]\")\n            print(f\"Input Tensor Shape:  {x_last.shape}\")\n        out_sppf = self.sppf(x_last)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {out_sppf.shape}\")\n\n            print(\"[Layer: Upsample 10]\")\n            print(f\"Input Tensor Shape:  {out_sppf.shape}\")\n        x = self.upsample1(out_sppf)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 11]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {x_second.shape}\")\n        x = torch.cat((x,x_second), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 12]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        conc1 = self.c2f_block1(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {conc1.shape}\")\n\n            print(\"[Layer: Upsample 13]\")\n            print(f\"Input Tensor Shape:  {conc1.shape}\")\n        x = self.upsample2(conc1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 14]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {x_first.shape}\")\n        x = torch.cat((x,x_first), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 15]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det1 = self.c2f_block2(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det1.shape}\")\n\n            print(\"[Layer: Conv 16]\")\n            print(f\"Input Tensor Shape:  {det1.shape}\")\n        x = self.conv1(det1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 17]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {conc1.shape}\")\n        x = torch.cat((x,conc1), dim=1)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 18]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det2 = self.c2f_block3(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det2.shape}\")\n\n            print(\"[Layer: Conv 19]\")\n            print(f\"Input Tensor Shape:  {det2.shape}\")\n        x = self.conv2(det2)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: Concat 20]\")\n            print(f\"Input Tensor Shape:  {x.shape}, {out_sppf.shape}\")\n        x = torch.cat((x,out_sppf), dim=1)\n       \n        if DEBUG:\n            print(f\"Output Tensor Shape: {x.shape}\")\n\n            print(\"[Layer: C2f 21]\")\n            print(f\"Input Tensor Shape:  {x.shape}\")\n        det3 = self.c2f_block4(x)\n        if DEBUG:\n            print(f\"Output Tensor Shape: {det3.shape}\")\n        \n        return det1, det2, det3\n    \nclass Head(nn.Module):\n    def __init__(self):\n        super(Head, self).__init__()\n        \n        self.det1 = DetectBlock(c=256)\n        self.det2 = DetectBlock(c=512)\n        self.det3 = DetectBlock(c=512)\n        \n    def forward(self, x1, x2, x3):\n        return self.det1(x1), self.det2(x2), self.det3(x3)\n    \n    \nclass YOLO(nn.Module):\n    def __init__(self):\n        super(YOLO, self).__init__()\n        self.h1 = BackBone()\n        self.h2 = Neck()\n        self.h3 = Head()\n        \n    def forward(self, x): \n        \n        if DEBUG:\n            print(\"---------- Backbone ----------\")\n            print(\"[Backbone Input]\")\n            print(f\"Input Tensor Shape: {x.shape}\")\n        res1, res2, res3 = self.h1(x)\n        if DEBUG:\n            print(\"[Backbone Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n            print(\"------------------------------\")\n\n        if DEBUG:\n            print(\"---------- Neck ----------\")\n            print(\"[Neck Input]\")\n            print(f\"Input Tensor Shape:  \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n        det1, det2, det3 = self.h2(res1, res2, res3)\n        if DEBUG:\n            print(\"[Neck Output]\")\n            print(f\"Output Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n            print(\"------------------------------\")\n\n        if DEBUG:\n            print(\"---------- Head ----------\")\n            print(\"[Head Input]\")\n            print(f\"Input Tensor Shape: \\n\\t\\t      {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n        det1, det2, det3 = self.h3(det1, det2, det3)\n        if DEBUG:\n            print(\"[Head Output]\")\n            print(f\"Output Tensor Bbox Loss: \\n\\t\\t     {det1[0].shape}, \\n\\t\\t     {det2[0].shape}, \\n\\t\\t     {det3[0].shape}\")\n            print(f\"Output Tensor Cls Loss: \\n\\t\\t     {det1[1].shape}, \\n\\t\\t     {det2[1].shape}, \\n\\t\\t     {det3[1].shape}\")\n            print(\"------------------------------\")\n\n        return det1, det2, det3\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:28:44.050222Z","iopub.execute_input":"2024-08-07T06:28:44.050698Z","iopub.status.idle":"2024-08-07T06:28:44.116699Z","shell.execute_reply.started":"2024-08-07T06:28:44.050668Z","shell.execute_reply":"2024-08-07T06:28:44.115697Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Initialize dataset and dataloader\n\nDEBUG = True\nnum_epochs = 1\n\ncsv_file = '/kaggle/input/data/train_solution_bounding_boxes (1).csv'\nimg_dir = '/kaggle/input/data/training_images'\ntransform = transforms.Compose([\n    #data aug \n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\nif DEBUG:\n    print(\"========================================\")\n    print(\"        YOLOv8 Model Debug Output       \")\n    print(\"========================================\")\nmodel = YOLO()\n\ndataset = YOLODataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Training loop\nbbox_loss_fn = nn.MSELoss()\ncls_loss_fn = nn.BCEWithLogitsLoss()\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    \n    for images, targets in dataloader:\n        images = images.to(device)\n        bbox_targets = [t.to(device) for t in targets['boxes']]\n        cls_targets = [t.to(device) for t in targets['labels']]\n\n        optimizer.zero_grad()\n\n        # Predizioni del modello\n        outputs = model(images)\n        \n        #large object\n        large = outputs[0]\n        large_box = large[0]\n\n        pooled_tensor = F.adaptive_avg_pool2d(large_box, (1, 1))\n        large_box = pooled_tensor.view(1,4)\n        large_cls = large[1]\n\n        pooled_tensor_cls = F.adaptive_avg_pool2d(large_cls, (1, 1))\n        large_cls = pooled_tensor_cls.view(1,1)\n        \n        #medium object \n        medium = outputs[1]\n        medium_box = medium[0]\n        pooled_tensor = F.adaptive_avg_pool2d(medium_box, (1, 1))\n        medium_box = pooled_tensor.view(1, 4)\n        medium_cls = medium[1]\n        pooled_tensor_cls = F.adaptive_avg_pool2d(medium_cls, (1, 1))\n        medium_cls = pooled_tensor_cls.view(1,1)\n        \n        #small object\n        small = outputs[2]\n        small_box = small[0]\n        pooled_tensor = F.adaptive_avg_pool2d(small_box, (1, 1))\n        small_box = pooled_tensor.view(1, 4)\n        small_cls = small[1]\n        pooled_tensor_cls = F.adaptive_avg_pool2d(small_cls, (1, 1))\n        small_cls = pooled_tensor_cls.view(1,1)\n        \n        bbox_preds = [large_box, medium_box, small_box]\n        cls_preds = [large_cls, medium_cls, small_cls]\n        \n\n        # Calcolo delle perdite\n        bbox_loss = 0.0\n        cls_loss = 0.0\n       \n\n        #for i in range(len(bbox_preds)):\n            #bbox_loss += bbox_loss_fn(bbox_preds[i], bbox_targets[0])\n            #cls_loss += cls_loss_fn(cls_preds[i], cls_targets[0].unsqueeze(1).float())\n\n        total_loss = bbox_loss + cls_loss\n        break\n        \n        total_loss.backward()\n        optimizer.step()\n\n        epoch_loss += total_loss.item()\n    \n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:28:44.118438Z","iopub.execute_input":"2024-08-07T06:28:44.118872Z","iopub.status.idle":"2024-08-07T06:28:44.817520Z","shell.execute_reply.started":"2024-08-07T06:28:44.118841Z","shell.execute_reply":"2024-08-07T06:28:44.816439Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"========================================\n        YOLOv8 Model Debug Output       \n========================================\n---------- Backbone ----------\n[Backbone Input]\nInput Tensor Shape: torch.Size([1, 3, 128, 128])\n[Layer: Conv 0]\nInput Tensor Shape:  torch.Size([1, 3, 128, 128])\nOutput Tensor Shape: torch.Size([1, 64, 64, 64])\n[Layer: Conv 1]\nInput Tensor Shape:  torch.Size([1, 64, 64, 64])\nOutput Tensor Shape: torch.Size([1, 128, 32, 32])\n[Layer: C2f 2]\nInput Tensor Shape:  torch.Size([1, 128, 32, 32])\nOutput Tensor Shape: torch.Size([1, 128, 32, 32])\n[Layer: Conv 3]\nInput Tensor Shape:  torch.Size([1, 128, 32, 32])\nOutput Tensor Shape: torch.Size([1, 256, 16, 16])\n[Layer: C2f 4]\nInput Tensor Shape:  torch.Size([1, 256, 16, 16])\nOutput Tensor Shape: torch.Size([1, 256, 16, 16])\n[Layer: Conv 5]\nInput Tensor Shape:  torch.Size([1, 256, 16, 16])\nOutput Tensor Shape: torch.Size([1, 512, 8, 8])\n[Layer: C2f 6]\nInput Tensor Shape:  torch.Size([1, 512, 8, 8])\nOutput Tensor Shape: torch.Size([1, 512, 8, 8])\n[Layer: Conv 7]\nInput Tensor Shape:  torch.Size([1, 512, 8, 8])\nOutput Tensor Shape: torch.Size([1, 512, 4, 4])\n[Layer: C2f 8]\nInput Tensor Shape:  torch.Size([1, 512, 4, 4])\nOutput Tensor Shape: torch.Size([1, 512, 4, 4])\n[Backbone Output]\nOutput Tensor Shape: \n\t\t     torch.Size([1, 256, 16, 16]), \n\t\t     torch.Size([1, 512, 8, 8]), \n\t\t     torch.Size([1, 512, 4, 4])\n------------------------------\n---------- Neck ----------\n[Neck Input]\nInput Tensor Shape:  \n\t\t     torch.Size([1, 256, 16, 16]), \n\t\t     torch.Size([1, 512, 8, 8]), \n\t\t     torch.Size([1, 512, 4, 4])\n[Layer: SPPF 9]\nInput Tensor Shape:  torch.Size([1, 512, 4, 4])\nOutput Tensor Shape: torch.Size([1, 512, 4, 4])\n[Layer: Upsample 10]\nInput Tensor Shape:  torch.Size([1, 512, 4, 4])\nOutput Tensor Shape: torch.Size([1, 512, 8, 8])\n[Layer: Concat 11]\nInput Tensor Shape:  torch.Size([1, 512, 8, 8]), torch.Size([1, 512, 8, 8])\nOutput Tensor Shape: torch.Size([1, 1024, 8, 8])\n[Layer: C2f 12]\nInput Tensor Shape:  torch.Size([1, 1024, 8, 8])\nOutput Tensor Shape: torch.Size([1, 512, 8, 8])\n[Layer: Upsample 13]\nInput Tensor Shape:  torch.Size([1, 512, 8, 8])\nOutput Tensor Shape: torch.Size([1, 512, 16, 16])\n[Layer: Concat 14]\nInput Tensor Shape:  torch.Size([1, 512, 16, 16]), torch.Size([1, 256, 16, 16])\nOutput Tensor Shape: torch.Size([1, 768, 16, 16])\n[Layer: C2f 15]\nInput Tensor Shape:  torch.Size([1, 768, 16, 16])\nOutput Tensor Shape: torch.Size([1, 256, 16, 16])\n[Layer: Conv 16]\nInput Tensor Shape:  torch.Size([1, 256, 16, 16])\nOutput Tensor Shape: torch.Size([1, 256, 8, 8])\n[Layer: Concat 17]\nInput Tensor Shape:  torch.Size([1, 256, 8, 8]), torch.Size([1, 512, 8, 8])\nOutput Tensor Shape: torch.Size([1, 768, 8, 8])\n[Layer: C2f 18]\nInput Tensor Shape:  torch.Size([1, 768, 8, 8])\nOutput Tensor Shape: torch.Size([1, 512, 8, 8])\n[Layer: Conv 19]\nInput Tensor Shape:  torch.Size([1, 512, 8, 8])\nOutput Tensor Shape: torch.Size([1, 512, 4, 4])\n[Layer: Concat 20]\nInput Tensor Shape:  torch.Size([1, 512, 4, 4]), torch.Size([1, 512, 4, 4])\nOutput Tensor Shape: torch.Size([1, 1024, 4, 4])\n[Layer: C2f 21]\nInput Tensor Shape:  torch.Size([1, 1024, 4, 4])\nOutput Tensor Shape: torch.Size([1, 512, 4, 4])\n[Neck Output]\nOutput Tensor Shape: \n\t\t     torch.Size([1, 256, 16, 16]), \n\t\t     torch.Size([1, 512, 8, 8]), \n\t\t     torch.Size([1, 512, 4, 4])\n------------------------------\n---------- Head ----------\n[Head Input]\nInput Tensor Shape: \n\t\t      torch.Size([1, 256, 16, 16]), \n\t\t     torch.Size([1, 512, 8, 8]), \n\t\t     torch.Size([1, 512, 4, 4])\n[Detect:]\nInput: torch.Size([1, 256, 16, 16])\n\t [Conv]\nOutput: torch.Size([1, 64, 16, 16])\n\t [Conv]\nOutput: torch.Size([1, 64, 16, 16])\n\t [Conv2D]\nConv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\nOutput: torch.Size([1, 4, 16, 16])\n[Detect:]\nInput: torch.Size([1, 512, 8, 8])\n\t [Conv]\nOutput: torch.Size([1, 64, 8, 8])\n\t [Conv]\nOutput: torch.Size([1, 64, 8, 8])\n\t [Conv2D]\nConv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\nOutput: torch.Size([1, 4, 8, 8])\n[Detect:]\nInput: torch.Size([1, 512, 4, 4])\n\t [Conv]\nOutput: torch.Size([1, 64, 4, 4])\n\t [Conv]\nOutput: torch.Size([1, 64, 4, 4])\n\t [Conv2D]\nConv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\nOutput: torch.Size([1, 4, 4, 4])\n[Head Output]\nOutput Tensor Bbox Loss: \n\t\t     torch.Size([1, 4, 16, 16]), \n\t\t     torch.Size([1, 4, 8, 8]), \n\t\t     torch.Size([1, 4, 4, 4])\nOutput Tensor Cls Loss: \n\t\t     torch.Size([1, 1, 16, 16]), \n\t\t     torch.Size([1, 1, 8, 8]), \n\t\t     torch.Size([1, 1, 4, 4])\n------------------------------\nEpoch 1/1, Loss: 0.0\n","output_type":"stream"}]}]}