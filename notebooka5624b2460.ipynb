{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3866417,"sourceType":"datasetVersion","datasetId":843852}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:44:23.641069Z","iopub.execute_input":"2024-08-04T15:44:23.641672Z","iopub.status.idle":"2024-08-04T15:44:36.034411Z","shell.execute_reply.started":"2024-08-04T15:44:23.641625Z","shell.execute_reply":"2024-08-04T15:44:36.033471Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class YOLODataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n        # Check if dataset is empty\n        if len(self.img_labels) == 0:\n            raise ValueError(\"Dataset is empty. Please check the CSV file and the image directory.\")\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"Image file {img_path} not found.\")\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Extract bounding box and class probabilities\n        labels = self.img_labels.iloc[idx, 1:].values.astype(float)\n        labels = torch.tensor(labels, dtype=torch.float32)  # Convert labels to Float32\n\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            labels = self.target_transform(labels)\n\n        return {'image': image, 'label': labels}","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:44:36.036040Z","iopub.execute_input":"2024-08-04T15:44:36.036421Z","iopub.status.idle":"2024-08-04T15:44:36.045241Z","shell.execute_reply.started":"2024-08-04T15:44:36.036397Z","shell.execute_reply":"2024-08-04T15:44:36.044358Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, k, s, p, c=3, dim=64, mc=512, w=1):\n        super(ConvBlock, self).__init__()\n        print(f\"out_channels = {min(int(dim),mc)*w}\")\n        dim = int(dim)\n        self.conv = nn.Conv2d(in_channels=c, out_channels=min(dim,mc)*w, kernel_size=k, stride=s, padding=p)\n        self.batch_norm = nn.BatchNorm2d(num_features=min(dim,mc)*w)\n        self.activation = nn.SiLU()\n    \n    def forward(self, x):\n        print(\"\\t \\t \\t \\t [ConvBlock]\")\n        print(f\"\\t \\t \\t \\t Input: x = {x.shape}\")\n        x = self.conv(x)\n        print(f\"\\t \\t \\t \\t Output: x = {x.shape}\")\n        x = self.batch_norm(x)\n        x = self.activation(x)\n        return x\n    \n\nclass Bottleneck(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, dim=64, shortcut=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = ConvBlock(k,s,p,c,dim=dim)\n        self.conv2 = ConvBlock(k,s,p,c,dim=dim)\n        self.short = shortcut\n    \n    def forward(self, x):\n        if self.short: \n            print(f\"\\t \\t \\t [Conv1]\")\n            print(f\"\\t \\t \\t Input: x = {x.shape}\")\n            res = self.conv1(x)\n            print(f\"\\t \\t \\t Output: x = {res.shape}\")\n            res = self.conv2(res)\n            return x + res\n        else: \n            res = self.conv1(x)\n            res = self.conv2(x)\n            return res \n            \nclass C2fBlock(nn.Module):\n    def __init__(self, k=1, s=1, p=0, c=3, depth_multiple=1, shortcut=True, dim=64, mc=512, w=1):\n        super(C2fBlock, self).__init__() \n        self.conv1 = ConvBlock(k,s,p,c,dim=dim,mc=mc,w=w)\n        half_dim = dim / 2\n        print(f\"half_dim = {half_dim}\")\n        self.bottlenecks = nn.ModuleList([Bottleneck(k=3,s=1,p=1,c=c,dim=half_dim) for _ in range(depth_multiple)])\n        self.conv2 = ConvBlock(k,s,p,c,dim=dim,mc=mc,w=w)\n    \n    def forward(self, x):\n        \n        print(\"\\t [C2fBlock]\")\n        print(\"\\t \\t [Conv1]\")\n        print(f\"\\t \\t Input: x = {x.shape}\")\n        x = self.conv1(x)\n        print(f\"\\t \\t Output: x = {x.shape}\")\n        # Split the input tensor into two halves along the channel dimension\n        print(\"\\t \\t [Split]\")\n        print(f\"\\t \\t Input: x = {x.shape}\")\n        x1, x2 = torch.split(x, x.size(1) // 2, dim=1)\n        print(f\"\\t \\t Output: x1 = {x1.shape}, x2 = {x2.shape}\")\n        \n        \n        # Process the other half (x2) through the bottlenecks\n        bottleneck_outputs = []\n        # append half of the input before processing\n        bottleneck_outputs.append(x2.clone())\n        print(\"\\t \\t [Bottlenecks]\")\n        print(f\"\\t \\t Input: x2 = {x2.shape}\")\n        for bott in self.bottlenecks:\n            x2 = bott(x2)\n            bottleneck_outputs.append(x2.clone())\n            \n        # this will concatenate half of the input before processing\n        # and after each bottleneck processing  \n        concatenated_bottleneck_outputs = torch.cat(bottleneck_outputs, dim=1)\n\n        # add the other half\n        x = torch.cat((x1, concatenated_bottleneck_outputs), dim=1)\n        \n        x = self.conv2(x)\n        return x\n    \nclass SPPF(nn.Module):\n    def __init__(self, k=3, s=1, p=0, c=3, dim=64):\n        super(SPPF, self).__init__() \n        \n        #Le dimensioni dei pool sono sicuramente da modificare\n        self.conv1 = ConvBlock(k=k,s=s,p=0,c=c,dim=dim)\n        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n        self.pool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=4)\n        self.pool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=6)\n        self.conv2 = ConvBlock(k=k,s=s,p=1,c=c,dim=dim)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        pool1 = self.pool1(x)\n        pool2 = self.pool2(x)\n        pool3 = self.pool3(x)\n        x = torch.cat([x, pool1, pool2, pool3], dim=1)\n        x = self.conv2(x)\n        return x\n    \nclass DetectBlock(nn.Module):\n    def __init__(self, k=3, s=1, p=1, c=3, reg_max=16, nc=1, mc=512, w=1):\n        super(DetectBlock, self).__init__()\n        \n        #reg_max = controlla la precisione della regression sulla boundy box \n        #nc = number of classes\n        self.box_conv1 = ConvBlock(k,s,p,c)\n        self.box_conv2 = ConvBlock(k,s,p,c)\n        self.box_conv3 = nn.Conv2d(in_channels=c, out_channels=4*reg_max, kernel_size=k, stride=1, padding=0)\n        \n        self.class_conv1 = ConvBlock(k,s,p,c)\n        self.class_conv2 = ConvBlock(k,s,p,c)\n        self.class_conv3 = nn.Conv2d(in_channels=c, out_channels=nc, kernel_size=k, stride=1, padding=0)\n        \n    def forward(self, x): \n        ret1 = self.box_conv1(x)\n        ret1 = self.box_conv2(ret1)\n        ret1 = self.box_conv3(ret1)\n        \n        ret2 = self.class_conv1(x)\n        ret2 = self.class_conv2(ret2)\n        ret2 = self.class_conv3(ret2)\n        \n        return ret1, ret2\n               \nclass BackBone(nn.Module):\n    def __init__(self, k=3, s=2, p=1, depth=1):\n        super(BackBone, self).__init__()\n        \n        self.conv1 = ConvBlock(k,s,p)\n        self.conv2 = ConvBlock(k,s,p, dim=128, c=64)\n        self.c2f = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=128, c=128)\n        self.conv3 = ConvBlock(k,s,p, dim=256, c=128)\n        self.c2f_second = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=256, c=256)\n        self.conv4 = ConvBlock(k,s,p,dim=512, c=256)\n        self.c2f_third = C2fBlock(k=1,s=1,p=0,depth_multiple=6*depth,dim=512, c=512)\n        self.conv5 = ConvBlock(k,s,p,dim=1024, c=512)\n        self.c2f_last = C2fBlock(k=1,s=1,p=0,depth_multiple=3*depth,dim=1024, c=512)\n        \n    def forward(self, x):\n        \n        print(\"[Layer: Conv 0]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv1(x)\n        print(f\"Output Tensor Shape: {x.shape}\")\n    \n        print(\"[Layer: Conv 1]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.conv2(x)\n        print(f\"Output Tensor Shape: {x.shape}\")\n    \n        print(\"[Layer: C2f 2]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        x = self.c2f(x)\n        print(f\"Output Tensor Shape: {x.shape}\")\n    \n        print(\"[Layer: Conv 3]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")        \n        x = self.conv3(x)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: C2f 4]\")\n        print(f\"Input Tensor Shape:  {x.shape}\") \n        x_first = self.c2f_second(x)\n        print(f\"Output Tensor Shape: {x_first.shape}\")\n        \n        print(\"[Layer: Conv 5]\")\n        print(f\"Input Tensor Shape:  {x_first.shape}\") \n        x = self.conv4(x_first)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: C2f 6]\")\n        print(f\"Input Tensor Shape:  {x.shape}\") \n        x_second = self.c2f_third(x)\n        print(f\"Output Tensor Shape: {x_second.shape}\")\n        \n        print(\"[Layer: Conv 7]\")\n        print(f\"Input Tensor Shape:  {x_second.shape}\")\n        x = self.conv5(x_second)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: C2f 8]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        x_last = self.c2f_last(x)\n        print(f\"Output Tensor Shape: {x_last.shape}\")\n        \n        return x_first, x_second, x_last\n    \nclass Neck(nn.Module):\n    def __init__(self, depth=1, scale=2):\n        super(Neck, self).__init__()\n        \n        self.sppf = SPPF(k=1,dim=1024,c=512)\n        self.upsample1 = nn.Upsample(size=(24,24))\n        self.upsample2 = nn.Upsample(scale_factor=2)\n        self.c2f_block1 = C2fBlock(dim=512,c=512)\n        self.c2f_block2 = C2fBlock(dim=256,c=768) #256 + 512 check\n        self.c2f_block3 = C2fBlock(dim=512,c=768)\n        self.c2f_block4 = C2fBlock(dim=1024,c=512)\n        self.conv1 = ConvBlock(k=3,s=2,p=1,dim=256,c=256)\n        self.conv2 = ConvBlock(k=3,s=2,p=1,dim=512,c=512)\n        \n    def forward(self, x_first, x_second, x_last):\n        \n        print(\"[Layer: SPPF 9]\")\n        print(f\"Input Tensor Shape:  {x_last.shape}\")\n        out_sppf = self.sppf(x_last)\n        print(f\"Output Tensor Shape: {out_sppf.shape}\")\n        \n        print(\"[Layer: Upsample 10]\")\n        print(f\"Input Tensor Shape:  {out_sppf.shape}\")\n        x = self.upsample1(out_sppf)\n        print(f\"Output Tensor Shape: {x.shape}\")\n\n        print(\"[Layer: Concat 11]\")\n        print(f\"Input Tensor Shape:  {x.shape}, {x_second.shape}\")\n        x = torch.cat((x,x_second), dim=1)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: C2f 12]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        conc1 = self.c2f_block1(x)\n        print(f\"Output Tensor Shape: {conc1.shape}\")\n        \n        print(\"[Layer: Upsample 13]\")\n        print(f\"Input Tensor Shape:  {conc1.shape}\")\n        x = self.upsample2(conc1)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: Concat 14]\")\n        print(f\"Input Tensor Shape:  {x.shape}, {x_first.shape}\")\n        x = torch.cat((x,x_first), dim=1)\n        print(f\"Output Tensor Shape: {x.shape}\")\n\n        print(\"[Layer: C2f 15]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        det1 = self.c2f_block2(x)\n        print(f\"Output Tensor Shape: {det1.shape}\")\n\n        print(\"[Layer: Conv 16]\")\n        print(f\"Input Tensor Shape:  {det1.shape}\")\n        x = self.conv1(det1)\n        print(f\"Output Tensor Shape: {x.shape}\")\n\n        print(\"[Layer: Concat 17]\")\n        print(f\"Input Tensor Shape:  {x.shape}, {conc1.shape}\")\n        x = torch.cat((x,conc1), dim=1)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: C2f 18]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        det2 = self.c2f_block3(x)\n        print(f\"Output Tensor Shape: {det2.shape}\")\n        \n        print(\"[Layer: Conv 19]\")\n        print(f\"Input Tensor Shape:  {det2.shape}\")\n        x = self.conv2(det2)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: Concat 20]\")\n        print(f\"Input Tensor Shape:  {x.shape}, {out_sppf.shape}\")\n        x = torch.cat((x,out_sppf), dim=1)\n        print(f\"Output Tensor Shape: {x.shape}\")\n        \n        print(\"[Layer: C2f 21]\")\n        print(f\"Input Tensor Shape:  {x.shape}\")\n        det3 = self.c2f_block4(x)\n        print(f\"Output Tensor Shape: {det3.shape}\")\n        \n        return det1, det2, det3\n    \nclass Head(nn.Module):\n    def __init__(self):\n        super(Head, self).__init__()\n        \n        self.det1 = DetectBlock()\n        self.det2 = DetectBlock()\n        self.det3 = DetectBlock()\n        \n    def forward(self, x1, x2, x3):\n        return self.det1(x1), self.det2(x2), self.det3(x3)\n    \n    \nclass YOLO(nn.Module):\n    def __init__(self):\n        super(YOLO, self).__init__()\n        self.h1 = BackBone()\n        self.h2 = Neck()\n        self.h3 = Head()\n        \n    def forward(self, x): \n        \n        print(\"---------- Backbone ----------\")\n        print(\"[Backbone Input]\")\n        print(f\"Input Tensor Shape: {x.shape}\")\n        res1, res2, res3 = self.h1(x)\n        print(\"[Backbone Output]\")\n        print(f\"Output Tensor Shape: \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n        print(\"------------------------------\")\n        \n        print(\"---------- Neck ----------\")\n        print(\"[Neck Input]\")\n        print(f\"Input Tensor Shape:  \\n\\t\\t     {res1.shape}, \\n\\t\\t     {res2.shape}, \\n\\t\\t     {res3.shape}\")\n        det1, det2, det3 = self.h2(res1, res2, res3)\n        print(\"[Neck Output]\")\n        print(f\"Output Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n        print(\"------------------------------\")\n        \n        print(\"---------- Head ----------\")\n        print(\"[Head Input]\")\n        print(f\"Input Tensor Shape: \\n\\t\\t      {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n        det1, det2, det3 = self.h3(det1, det2, det3)\n        print(\"[Head Output]\")\n        print(f\"Output Tensor Shape: \\n\\t\\t     {det1.shape}, \\n\\t\\t     {det2.shape}, \\n\\t\\t     {det3.shape}\")\n        print(\"------------------------------\")\n        \n        return det1, det2, det3\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:47:15.466386Z","iopub.execute_input":"2024-08-04T16:47:15.466729Z","iopub.status.idle":"2024-08-04T16:47:15.524018Z","shell.execute_reply.started":"2024-08-04T16:47:15.466704Z","shell.execute_reply":"2024-08-04T16:47:15.523289Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Initialize dataset and dataloader\n\ncsv_file = '/kaggle/input/data/train_solution_bounding_boxes (1).csv'\nimg_dir = '/kaggle/input/data/training_images'\ntransform = transforms.Compose([\n    #data aug \n    transforms.Resize((380, 380)),\n    transforms.ToTensor(),\n])\n\nprint(\"========================================\")\nprint(\"        YOLOv8 Model Debug Output       \")\nprint(\"========================================\")\nmodel = YOLO()\n\ndataset = YOLODataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, batch in enumerate(dataloader):\n        images = batch['image'].to(device)\n        targets = batch['label'].to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        \n        print(outputs.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:47:18.006697Z","iopub.execute_input":"2024-08-04T16:47:18.007015Z","iopub.status.idle":"2024-08-04T16:47:19.275644Z","shell.execute_reply.started":"2024-08-04T16:47:18.006991Z","shell.execute_reply":"2024-08-04T16:47:19.274054Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"========================================\n        YOLOv8 Model Debug Output       \n========================================\nout_channels = 64\nout_channels = 128\nout_channels = 128\nhalf_dim = 64.0\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 128\nout_channels = 256\nout_channels = 256\nhalf_dim = 128.0\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 128\nout_channels = 256\nout_channels = 512\nout_channels = 512\nhalf_dim = 256.0\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 256\nout_channels = 512\nout_channels = 512\nout_channels = 512\nhalf_dim = 512.0\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 512\nhalf_dim = 256.0\nout_channels = 256\nout_channels = 256\nout_channels = 512\nout_channels = 256\nhalf_dim = 128.0\nout_channels = 128\nout_channels = 128\nout_channels = 256\nout_channels = 512\nhalf_dim = 256.0\nout_channels = 256\nout_channels = 256\nout_channels = 512\nout_channels = 512\nhalf_dim = 512.0\nout_channels = 512\nout_channels = 512\nout_channels = 512\nout_channels = 256\nout_channels = 512\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\nout_channels = 64\n---------- Backbone ----------\n[Backbone Input]\nInput Tensor Shape: torch.Size([1, 3, 380, 380])\n[Layer: Conv 0]\nInput Tensor Shape:  torch.Size([1, 3, 380, 380])\n\t \t \t \t [ConvBlock]\n\t \t \t \t Input: x = torch.Size([1, 3, 380, 380])\n\t \t \t \t Output: x = torch.Size([1, 64, 190, 190])\nOutput Tensor Shape: torch.Size([1, 64, 190, 190])\n[Layer: Conv 1]\nInput Tensor Shape:  torch.Size([1, 64, 190, 190])\n\t \t \t \t [ConvBlock]\n\t \t \t \t Input: x = torch.Size([1, 64, 190, 190])\n\t \t \t \t Output: x = torch.Size([1, 128, 95, 95])\nOutput Tensor Shape: torch.Size([1, 128, 95, 95])\n[Layer: C2f 2]\nInput Tensor Shape:  torch.Size([1, 128, 95, 95])\n\t [C2fBlock]\n\t \t [Conv1]\n\t \t Input: x = torch.Size([1, 128, 95, 95])\n\t \t \t \t [ConvBlock]\n\t \t \t \t Input: x = torch.Size([1, 128, 95, 95])\n\t \t \t \t Output: x = torch.Size([1, 128, 95, 95])\n\t \t Output: x = torch.Size([1, 128, 95, 95])\n\t \t [Split]\n\t \t Input: x = torch.Size([1, 128, 95, 95])\n\t \t Output: x1 = torch.Size([1, 64, 95, 95]), x2 = torch.Size([1, 64, 95, 95])\n\t \t [Bottlenecks]\n\t \t Input: x2 = torch.Size([1, 64, 95, 95])\n\t \t \t [Conv1]\n\t \t \t Input: x = torch.Size([1, 64, 95, 95])\n\t \t \t \t [ConvBlock]\n\t \t \t \t Input: x = torch.Size([1, 64, 95, 95])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[39], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[38], line 302\u001b[0m, in \u001b[0;36mYOLO.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Backbone Input]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Tensor Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 302\u001b[0m res1, res2, res3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Backbone Output]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Tensor Shape: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres3\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[38], line 160\u001b[0m, in \u001b[0;36mBackBone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Layer: C2f 2]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Tensor Shape:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc2f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Tensor Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Layer: Conv 3]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[38], line 73\u001b[0m, in \u001b[0;36mC2fBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Input: x2 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bott \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottlenecks:\n\u001b[0;32m---> 73\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[43mbott\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     bottleneck_outputs\u001b[38;5;241m.\u001b[39mappend(x2\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# this will concatenate half of the input before processing\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# and after each bottleneck processing  \u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[38], line 34\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m [Conv1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Input: x = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Output: x = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(res)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[38], line 16\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m [ConvBlock]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Input: x = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Output: x = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 128, 3, 3], expected input[1, 64, 95, 95] to have 128 channels, but got 64 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [64, 128, 3, 3], expected input[1, 64, 95, 95] to have 128 channels, but got 64 channels instead","output_type":"error"}]}]}