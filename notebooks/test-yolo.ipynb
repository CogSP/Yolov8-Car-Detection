{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3866417,"sourceType":"datasetVersion","datasetId":843852},{"sourceId":9152173,"sourceType":"datasetVersion","datasetId":5528569},{"sourceId":92363,"sourceType":"modelInstanceVersion","modelInstanceId":77438,"modelId":102070},{"sourceId":92950,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":77919,"modelId":102518}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":22535.483161,"end_time":"2024-08-10T20:26:34.782076","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-08-10T14:10:59.298915","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reimplementation from scatch of YOLOv8 for Car Detection\n\nThis notebook contains the implementation of the Yolo-v8 architecture. We use it for car detection","metadata":{"papermill":{"duration":0.012822,"end_time":"2024-08-10T14:11:02.149832","exception":false,"start_time":"2024-08-10T14:11:02.137010","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## All the Imports","metadata":{"papermill":{"duration":0.012096,"end_time":"2024-08-10T14:11:02.174088","exception":false,"start_time":"2024-08-10T14:11:02.161992","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image, ImageDraw\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport math\nimport json\nimport random\nimport matplotlib.patches as patches\nimport sklearn","metadata":{"papermill":{"duration":9.19441,"end_time":"2024-08-10T14:11:11.380974","exception":false,"start_time":"2024-08-10T14:11:02.186564","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constants\nThe constants above are used to index the encoded target and output in order to retrieve the corresponding quantities.\n\n* `DELTA_X = 0`\n* `DELTA_Y = 1`\n* `WIDTH = 2`\n* `HEIGHT = 3`\n* `CONFIDENCE = 4`\n* `CLASS = 5`","metadata":{"papermill":{"duration":0.012123,"end_time":"2024-08-10T14:11:11.467994","exception":false,"start_time":"2024-08-10T14:11:11.455871","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DELTA_X = 0\nDELTA_Y = 1\nWIDTH = 2\nHEIGHT = 3\nCONFIDENCE = 4\nCLASS = 5","metadata":{"papermill":{"duration":0.020974,"end_time":"2024-08-10T14:11:11.501646","exception":false,"start_time":"2024-08-10T14:11:11.480672","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils Function\n\n* `scale_to_range`: TODO\n* `show_image_with_boxes`: TODO\n* `from_grid_coordinates_to_bbox`: TODO\n","metadata":{"papermill":{"duration":0.011991,"end_time":"2024-08-10T14:11:11.526041","exception":false,"start_time":"2024-08-10T14:11:11.514050","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def scale_to_range(bboxes, old_x=676, old_y=380, new_x = 128, new_y=128):\n    x_scale = new_x / old_x\n    y_scale = new_y / old_y\n    \n    for box in bboxes:\n        box[0] = int(np.round(box[0]*x_scale))\n        box[1] = int(np.round(box[1]*y_scale))\n        box[2] = int(np.round(box[2]*x_scale))\n        box[3] = int(np.round(box[3]*y_scale))\n    return bboxes","metadata":{"papermill":{"duration":0.022695,"end_time":"2024-08-10T14:11:11.560983","exception":false,"start_time":"2024-08-10T14:11:11.538288","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_in_grid_coordinates(x_center, y_center, width, height, size=\"large\"):\n\n    #print(f\"la nostra bounding box è x_center = {x_center}, y_center = {y_center}, width = {width}, height = {height}\\n\")\n\n    if size == \"small\":\n        number_of_cells = 16\n        n_pixel_per_grid = 8\n\n    if size == \"medium\":\n        number_of_cells = 8\n        n_pixel_per_grid = 16\n\n    if size == \"large\":\n        number_of_cells = 4\n        n_pixel_per_grid = 32\n\n    i = 0\n    for cell in range(number_of_cells * number_of_cells):\n        #print(f\"colonna = {cell}\\n\")\n        #print(f\"riga = {i}\\n\")\n\n        if cell % number_of_cells  == 0:\n            i += 1\n\n        x_a = n_pixel_per_grid * (cell % number_of_cells)\n        y_a = n_pixel_per_grid * (i-1) \n        #print(f\"x_a = {x_a}, y_a = {y_a}\\n\")\n\n        # if verifica se il centro ricade nella cella\n        #print(f\"controlliamo se il centro ricade tra x = {x_a} e x = {x_a + n_pixel_per_grid}...\\n\")\n        if x_center >= x_a and x_center <= (x_a + n_pixel_per_grid):\n            #print(f\"controlliamo se il centro ricade tra x = {y_a} e x = {y_a + n_pixel_per_grid}...\\n\")\n            if y_center >= y_a and y_center <= (y_a + n_pixel_per_grid):\n                #print(\"il centro ricade qua! Calcoliamo quantità!\\n\")\n                delta_x = (x_center - x_a) / n_pixel_per_grid\n                delta_y = (y_center - y_a) / n_pixel_per_grid\n                delta_width = width / 128\n                delta_height = height / 128 # 128 is the heigth and width of the image\n                #print(f\"delta_x = {delta_x}, delta_y = {delta_y}, delta_width = {delta_width}, delta_height = {delta_height}\")\n                confidence = 1 # 100% confidence that is a car\n                cl = 1 # 1 = car, 0 = nothing\n                column = cell\n                row = i-1\n                return delta_x, delta_y, delta_width, delta_height, confidence, cl, column%number_of_cells, row\n            else:\n                continue\n        else:\n              continue\n        print(\"[ERROR]: there is a box but we have not found the cell in which it lies\")\n\n","metadata":{"papermill":{"duration":0.045283,"end_time":"2024-08-10T14:11:11.618775","exception":false,"start_time":"2024-08-10T14:11:11.573492","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image_with_boxes(self, idx):\n        image, boxes_list = self[idx]\n        image_draw = ImageDraw.Draw(image)\n        \n        # Draw the bounding boxes\n        for box in boxes_list:\n            xmin, ymin, xmax, ymax = box\n            image_draw.rectangle([xmin, ymin, xmax, ymax], outline=\"red\", width=2)\n        \n        # Display the image\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"papermill":{"duration":0.024494,"end_time":"2024-08-10T14:11:11.655894","exception":false,"start_time":"2024-08-10T14:11:11.631400","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image_and_bbox(image, encoding_of_boxes, filename=None, training=False):\n    \n    image_np = None\n\n    # if a filename is not provided, a 128 x 128 image is displayed\n    if filename == None:\n        image = image[0]\n\n        if image.shape == torch.Size([3, 128, 128]):\n            image_np = image.permute(1, 2, 0).numpy()\n\n    # if a filename is provided, the 676 x 380 image is displayed\n    else:\n        img_path = os.path.join(\"/kaggle/input/car-object-detection/data/testing_images\", filename)\n    \n        if training == True:\n            img_path = os.path.join(\"/kaggle/input/car-object-detection/data/training_images\", filename)\n    \n        image = Image.open(img_path).convert(\"RGB\")\n        image_np = np.array(image)\n            \n    # Plot the image with bounding boxes\n    fig, ax = plt.subplots(1)\n    ax.imshow(image_np)\n    \n    bboxes = from_grid_coordinate_to_bbox(encoding_of_boxes)\n\n    for box in bboxes:\n        \n        #print(f\"box = {box}\")\n\n        x_center = box[0]\n        y_center = box[1]\n        width = box[2]\n        height = box[3]\n        \n        xmin = (2*x_center - width) / 2\n        ymin = (2*y_center - height) / 2\n        xmax = (2*x_center + width) / 2\n        ymax = (2*y_center + height) / 2\n        \n        rect = None\n        # if a filename is not provided, the image is 128 x 128 and so the box should be\n        if filename == None:\n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        \n        # if the image is 676 x 380, the rectangle must be scaled\n        else:\n            bbox = scale_to_range(bboxes=[[xmin, ymin, xmax, ymax]], old_x=128, old_y=128, new_x=676, new_y=380)\n            bbox = bbox[0]\n            xmin = bbox[0]\n            ymin = bbox[1]\n            xmax = bbox[2]\n            ymax = bbox[3]\n            width = xmax - xmin\n            height = ymax - ymin\n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        \n        \n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        \n        \n        confidence_score = box[4]\n        ax.text(xmin, ymin - 5, f'{confidence_score:.4f}', color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n\n    \n    plt.axis('off')  # Turn off axis\n    plt.show()\n    \n    return ","metadata":{"papermill":{"duration":0.033378,"end_time":"2024-08-10T14:11:12.414210","exception":false,"start_time":"2024-08-10T14:11:12.380832","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def from_grid_coordinate_to_bbox(output, size=\"large\"):\n        \n        if size == \"small\":\n            number_of_cells = 16\n            n_pixel_per_grid = 8\n        \n        if size == \"medium\":\n            number_of_cells = 8\n            n_pixel_per_grid = 16\n        \n        if size == \"large\":\n            number_of_cells = 4\n            n_pixel_per_grid = 32\n    \n        bboxes = []\n        for i in range(number_of_cells):\n            for j in range(number_of_cells):\n                \n                if output[0][CLASS][i][j] >= 0.6:\n                    delta_x = float(output[0][DELTA_X][i][j])\n                    delta_y = float(output[0][DELTA_Y][i][j])\n                    delta_w = float(output[0][WIDTH][i][j])\n                    delta_h = float(output[0][HEIGHT][i][j])\n\n                    x_a = n_pixel_per_grid * j\n                    y_a = n_pixel_per_grid * i\n\n                    x = delta_x * n_pixel_per_grid + x_a\n                    y = delta_y * n_pixel_per_grid + y_a\n                    w = delta_w * 128\n                    h = delta_h * 128\n                    confidence = float(output[0][CONFIDENCE][i][j])                 \n\n                    bbox = [x, y, w, h, confidence]\n                    print(f\"bbox = {bbox}, class = {output[0][CLASS][i][j]}\")\n\n                    bboxes.append(bbox)\n\n        return bboxes                 ","metadata":{"papermill":{"duration":0.031999,"end_time":"2024-08-10T14:11:12.462129","exception":false,"start_time":"2024-08-10T14:11:12.430130","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{"papermill":{"duration":0.074855,"end_time":"2024-08-10T20:26:30.180981","exception":false,"start_time":"2024-08-10T20:26:30.106126","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Load the Model ","metadata":{"papermill":{"duration":0.073862,"end_time":"2024-08-10T20:26:30.328688","exception":false,"start_time":"2024-08-10T20:26:30.254826","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = YOLO();\nmodel_load_path = '/kaggle/input/yolo_v8_700_epoch/pytorch/default/1/yolov8_new_700_epoch.pth'\nmodel.load_state_dict(torch.load(model_load_path));\nmodel.to(device);\nmodel.eval();  # Set the model to evaluation mode\nprint(f\"model loaded: {model_load_path}\")","metadata":{"papermill":{"duration":0.082913,"end_time":"2024-08-10T20:26:30.484982","exception":false,"start_time":"2024-08-10T20:26:30.402069","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Parse the annotated test set","metadata":{}},{"cell_type":"code","source":"# Load the JSON data\ntest_annotation_path = '/kaggle/input/test-annotation-car-detection-dataset/2D-on-2D_annotations_export.json'\n  \n# Open the file and read its contents\nwith open(test_annotation_path, 'r') as file:\n    data = json.load(file)\n\ntest_image_boxes = {}\n\n# Iterate through each image entry in the JSON\nfor image_entry in data['images']:\n    image_name = image_entry['image']\n    annotations = image_entry.get('annotations', [])\n    \n    # Initialize a list to store bounding boxes for the current image\n    boxes = []\n    \n    # Iterate through each annotation\n    for annotation in annotations:\n        bbox = annotation['boundingBox']\n\n        # Extract x, y, width, and height\n        x_min = bbox['x']\n        y_min = bbox['y']\n        w = bbox['width']\n        h = bbox['height']\n        \n        x_max = x_min + w\n        y_max = y_min + h\n        \n        x_center = (x_min + x_max) / 2.0\n        y_center = (y_min + y_max) / 2.0\n    \n    \n        confidence = 1\n        \n        # Append the bounding box to the list\n        boxes.append([x_center, y_center, w, h, confidence])\n    \n    # convert in 128 x 128\n    boxes = scale_to_range(bboxes=boxes)\n     \n    number_of_cells = 4\n    encoded_result = torch.zeros(6, number_of_cells, number_of_cells)\n    for box in boxes:\n        delta_x, delta_y, delta_width, delta_height, confidence, cl, column, row = transform_in_grid_coordinates(x_center=box[0], y_center=box[1], width=box[2], height=box[3])\n    \n        box = [delta_x, delta_y, delta_width, delta_height, confidence, cl]\n        \n        encoded_result[0][row][column] = box[0]\n        encoded_result[1][row][column] = box[1]\n        encoded_result[2][row][column] = box[2]\n        encoded_result[3][row][column] = box[3]\n        encoded_result[4][row][column] = box[4]\n        encoded_result[5][row][column] = box[5]\n\n    # Store the list of bounding boxes in the dictionary\n    test_image_boxes[image_name] = encoded_result.unsqueeze(0)\n\n\n#for filename in test_image_boxes:\n#    show_image_and_bbox(image=None, encoding_of_boxes=test_image_boxes[filename], filename=filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test the Model\n\n#### mean Average Precision (mAP)\n\nTODO: insert latex formula","metadata":{"papermill":{"duration":0.073574,"end_time":"2024-08-10T20:26:30.632033","exception":false,"start_time":"2024-08-10T20:26:30.558459","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_cls_from_encode(encoded, threshold = 0.5):\n    cls_list = []\n    number_of_cells = 4\n    for i in range(number_of_cells):\n        for j in range(number_of_cells):\n            cls_list.append(encoded[0][CLASS][i][j])\n    cls_list = [\"positive\" if cls_score >= threshold else \"negative\" for cls_score in cls_list]\n    return cls_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def precision_recall_curve(target, out, thresholds):\n    precisions = []\n    recalls = []\n    \n    for threshold in thresholds:\n        # encoding: from cls get \"positive\" and \"negative\" using 0.5 as a threshold\n        y_pred = get_cls_from_encode(out, threshold=threshold)\n        y_true = get_cls_from_encode(target, threshold=threshold)\n        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\", zero_division=0)\n        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"positive\", zero_division=0)\n        \n        precisions.append(precision)\n        recalls.append(recall)\n\n    return precisions, recalls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_mAP(out, target, thresholds):\n    \n        precisions, recalls = precision_recall_curve(target=target, \n                                                 out=out,\n                                                 thresholds=thresholds)\n        precisions.append(1)\n        recalls.append(0)\n\n        precisions = np.array(precisions)\n        recalls = np.array(recalls)\n\n        AP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\n        return AP, precisions, recalls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds = np.arange(start=0.2, stop=0.7, step=0.05)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAINING == False and TRAINING_DEBUG == False:\n    \n    # Testing \n    test_transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n\n    test_img_dir = '/kaggle/input/car-object-detection/data/testing_images'\n    file_list = os.listdir(test_img_dir)\n    TEST_SET_SIZE = len(file_list)\n\n    random_number = random.randint(0, TEST_SET_SIZE)\n\n    filename = None\n    \n    image = None\n    target = None\n    for i in range(TEST_SET_SIZE):\n        if i == random_number:\n            filename = file_list[i]\n            print(f\"filename = {filename} randomly chosen\")\n            \n            target = test_image_boxes[filename]\n\n            img_path = os.path.join(test_img_dir, filename)\n\n            image = Image.open(img_path).convert(\"RGB\")\n\n            break \n            \n\n    image = test_transform(image)\n    image = image.unsqueeze(0)\n    image = image.to(device)\n\n    model.eval()\n\n    with torch.no_grad():\n        \n        out = model(image)\n        \n        AP, precisions, recalls = compute_mAP(out=out, target=target, thresholds=thresholds)\n        \n        plt.plot(recalls, precisions, linewidth=4, color=\"red\")\n        plt.xlabel(\"Recall\", fontsize=12, fontweight='bold')\n        plt.ylabel(\"Precision\", fontsize=12, fontweight='bold')\n        plt.title(\"Precision-Recall Curve\", fontsize=15, fontweight=\"bold\")\n        plt.show()\n        \n      \n        # Add a small epsilon to avoid division by zero, but handle the cases where both are zero\n        epsilon = 1e-10\n        f1_scores = 2 * ((precisions * recalls) / (precisions + recalls + epsilon))\n\n        # Set F1 score to 0 where both precision and recall are 0 (instead of NaN)\n        f1_scores[(precisions == 0) & (recalls == 0)] = 0\n\n        \n        # giving filename to the function, it displays the result in 676 x 380\n        # without it, it displays it in 128 x 128\n        \n        # if training = true you want to use images from the training set\n        \n        show_image_and_bbox(image.cpu(), out, filename, training=False)\n        show_image_and_bbox(image.cpu(), target, filename, training=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}